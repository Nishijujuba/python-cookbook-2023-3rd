{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishijujuba/python-cookbook-2023-3rd/blob/master/_downloads/8eed7e178f8fa30798f280ea82ff468b/autograd_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ozi5WGVEIv8O"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://docs.pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly2Ye_K_Iv8Q"
      },
      "source": [
        "A Gentle Introduction to `torch.autograd`\n",
        "=========================================\n",
        "\n",
        "`torch.autograd` is PyTorch's automatic differentiation engine that\n",
        "powers neural network training. In this section, you will get a\n",
        "conceptual understanding of how autograd helps a neural network train.\n",
        "\n",
        "Background\n",
        "----------\n",
        "\n",
        "Neural networks (NNs) are a collection of nested functions that are\n",
        "executed on some input data. These functions are defined by *parameters*\n",
        "(consisting of weights and biases), which in PyTorch are stored in\n",
        "tensors.\n",
        "\n",
        "Training a NN happens in two steps:\n",
        "\n",
        "**Forward Propagation**: In forward prop, the NN makes its best guess\n",
        "about the correct output. It runs the input data through each of its\n",
        "functions to make this guess.\n",
        "\n",
        "**Backward Propagation**: In backprop, the NN adjusts its parameters\n",
        "proportionate to the error in its guess. It does this by traversing\n",
        "backwards from the output, collecting the derivatives of the error with\n",
        "respect to the parameters of the functions (*gradients*), and optimizing\n",
        "the parameters using gradient descent. For a more detailed walkthrough\n",
        "of backprop, check out this [video from\n",
        "3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8).\n",
        "\n",
        "Usage in PyTorch\n",
        "----------------\n",
        "\n",
        "Let\\'s take a look at a single training step. For this example, we load\n",
        "a pretrained resnet18 model from `torchvision`. We create a random data\n",
        "tensor to represent a single image with 3 channels, and height & width\n",
        "of 64, and its corresponding `label` initialized to some random values.\n",
        "Label in pretrained models has shape (1,1000).\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>This tutorial works only on the CPU and will not work on GPU devices (even if tensors are moved to CUDA).</p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "65Yp0gEiIv8S",
        "outputId": "ea314e47-153a-41b3-928c-b4f1c92b0115",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 125MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "data = torch.rand(1, 3, 64, 64)\n",
        "labels = torch.rand(1, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n",
        "\n"
      ],
      "metadata": {
        "id": "iAHvcDAOK8mt",
        "outputId": "0873bae6-c978-4152-db6c-0c3e54a76c30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)\n"
      ],
      "metadata": {
        "id": "ronSv3k-LI_b",
        "outputId": "ad18070c-f3bb-49b6-8f6e-8784fc759881",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0.6064, 0.7566, 0.7164,  ..., 0.2291, 0.2338, 0.9033],\n",
            "          [0.3737, 0.9530, 0.4079,  ..., 0.2948, 0.0152, 0.8009],\n",
            "          [0.5868, 0.9241, 0.6795,  ..., 0.3566, 0.8000, 0.6773],\n",
            "          ...,\n",
            "          [0.6289, 0.9196, 0.9750,  ..., 0.3969, 0.3064, 0.9416],\n",
            "          [0.7567, 0.6302, 0.6577,  ..., 0.7207, 0.3188, 0.6220],\n",
            "          [0.3605, 0.0696, 0.9460,  ..., 0.9647, 0.8787, 0.7182]],\n",
            "\n",
            "         [[0.7879, 0.5128, 0.6273,  ..., 0.0077, 0.4611, 0.8532],\n",
            "          [0.2390, 0.1883, 0.9053,  ..., 0.9471, 0.2342, 0.1629],\n",
            "          [0.1641, 0.3597, 0.6280,  ..., 0.7551, 0.2479, 0.7240],\n",
            "          ...,\n",
            "          [0.1258, 0.9277, 0.6533,  ..., 0.7723, 0.2672, 0.8815],\n",
            "          [0.3650, 0.7117, 0.4573,  ..., 0.1747, 0.3890, 0.7111],\n",
            "          [0.9609, 0.0476, 0.1487,  ..., 0.3277, 0.1957, 0.9579]],\n",
            "\n",
            "         [[0.1383, 0.2249, 0.9418,  ..., 0.4493, 0.3834, 0.2009],\n",
            "          [0.2985, 0.3353, 0.1312,  ..., 0.9082, 0.1380, 0.6555],\n",
            "          [0.7994, 0.8242, 0.2147,  ..., 0.8781, 0.9355, 0.7683],\n",
            "          ...,\n",
            "          [0.4720, 0.5141, 0.6302,  ..., 0.0717, 0.5471, 0.6208],\n",
            "          [0.8231, 0.3521, 0.6093,  ..., 0.6945, 0.0250, 0.4381],\n",
            "          [0.1539, 0.2879, 0.1436,  ..., 0.8842, 0.9900, 0.1758]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(labels)"
      ],
      "metadata": {
        "id": "iG7fGHreLMYO",
        "outputId": "d364fb18-62d4-46ea-effe-df4734c654d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[5.4359e-01, 5.2782e-01, 4.3972e-01, 3.7217e-01, 9.5360e-01, 7.3617e-01,\n",
            "         5.9428e-01, 1.8654e-01, 3.3698e-01, 6.9070e-01, 5.5069e-01, 6.0084e-01,\n",
            "         7.7408e-01, 7.9019e-01, 4.8164e-01, 4.3611e-01, 6.7691e-01, 1.3822e-01,\n",
            "         5.3003e-02, 3.8807e-01, 5.4047e-02, 6.2502e-01, 6.1881e-01, 1.1707e-01,\n",
            "         4.5454e-01, 8.3824e-01, 4.4360e-01, 2.1732e-03, 7.9705e-01, 7.2369e-01,\n",
            "         4.5937e-01, 5.8701e-01, 2.6559e-01, 1.3350e-02, 8.7201e-01, 2.0968e-01,\n",
            "         8.6884e-01, 8.1964e-01, 4.1572e-01, 5.9808e-01, 2.2318e-01, 3.3777e-01,\n",
            "         7.1876e-01, 7.1240e-01, 1.2244e-01, 1.1079e-01, 4.3274e-01, 8.1832e-01,\n",
            "         3.7218e-01, 5.1647e-01, 4.9659e-01, 3.9252e-01, 9.2870e-01, 1.3825e-01,\n",
            "         6.5420e-01, 9.5362e-01, 3.1945e-01, 4.1297e-01, 6.9258e-01, 9.5552e-02,\n",
            "         8.7889e-01, 4.9736e-01, 8.5048e-01, 5.5419e-01, 5.9285e-01, 5.4527e-01,\n",
            "         4.0390e-01, 9.7109e-01, 5.7211e-01, 7.1151e-02, 5.6616e-01, 7.5649e-01,\n",
            "         1.2708e-01, 8.5312e-01, 5.1595e-01, 5.0192e-01, 2.6850e-01, 8.0782e-01,\n",
            "         3.6983e-01, 5.7489e-01, 5.9078e-02, 8.7522e-01, 2.8045e-01, 1.7532e-01,\n",
            "         6.2310e-01, 5.2514e-01, 6.4878e-01, 8.0538e-01, 6.8627e-01, 9.5859e-01,\n",
            "         3.4376e-01, 9.7654e-01, 9.2528e-01, 6.7377e-01, 1.8470e-01, 3.2980e-01,\n",
            "         3.1843e-01, 1.3322e-01, 7.2436e-01, 5.6408e-01, 1.8478e-01, 3.5504e-01,\n",
            "         5.0096e-01, 9.5238e-01, 3.7220e-01, 5.1683e-02, 6.5079e-01, 5.7077e-01,\n",
            "         6.5236e-01, 7.6238e-01, 4.1310e-01, 1.8172e-01, 9.5557e-01, 1.1508e-01,\n",
            "         6.5039e-01, 9.5285e-01, 4.1282e-01, 8.0896e-01, 1.7870e-02, 9.0978e-01,\n",
            "         3.9484e-01, 3.9304e-01, 5.1550e-01, 5.2505e-01, 4.0854e-01, 7.4052e-01,\n",
            "         7.3627e-01, 5.2176e-01, 7.0564e-01, 9.8949e-02, 2.1346e-01, 9.2757e-04,\n",
            "         9.7190e-02, 1.7925e-01, 9.3592e-01, 2.2860e-01, 7.7690e-01, 9.2211e-01,\n",
            "         8.9730e-01, 3.0382e-01, 7.5970e-01, 5.7216e-01, 6.6336e-01, 2.6200e-02,\n",
            "         3.8439e-01, 6.8777e-01, 9.8895e-01, 9.6999e-01, 1.9661e-02, 4.6616e-01,\n",
            "         4.5951e-02, 3.8474e-01, 8.4908e-01, 7.8466e-01, 3.5840e-02, 3.6597e-01,\n",
            "         7.7923e-01, 5.9759e-01, 8.1087e-01, 1.4177e-01, 7.8261e-01, 1.0646e-02,\n",
            "         2.9604e-01, 6.0850e-01, 2.3422e-01, 3.6525e-01, 1.6484e-01, 3.6605e-01,\n",
            "         6.3093e-01, 9.1332e-01, 7.9312e-01, 5.6480e-02, 4.5117e-01, 6.1885e-01,\n",
            "         1.1720e-02, 2.1574e-01, 4.0159e-01, 5.0250e-01, 7.4303e-01, 2.1110e-01,\n",
            "         9.3206e-01, 4.1581e-01, 3.1762e-01, 2.6676e-01, 9.3914e-01, 7.8177e-01,\n",
            "         3.8889e-02, 6.1024e-01, 4.8041e-01, 5.0848e-01, 5.2681e-02, 6.5466e-02,\n",
            "         6.3250e-01, 7.8585e-01, 3.2186e-01, 2.6688e-01, 9.7035e-01, 4.7006e-01,\n",
            "         6.2440e-01, 2.7988e-01, 4.6649e-01, 9.6145e-01, 4.9774e-01, 3.3226e-01,\n",
            "         6.7702e-01, 7.8416e-01, 5.9805e-01, 7.8887e-02, 4.7459e-01, 6.6581e-01,\n",
            "         6.6479e-02, 9.5621e-01, 2.3569e-01, 9.0394e-01, 9.8059e-02, 3.7031e-01,\n",
            "         8.7697e-01, 8.4294e-01, 3.3736e-01, 1.6714e-02, 5.9651e-01, 2.7905e-01,\n",
            "         8.7424e-01, 1.6759e-01, 9.2555e-01, 3.6885e-01, 8.4958e-01, 1.9550e-01,\n",
            "         8.5138e-01, 7.8047e-01, 7.1664e-01, 2.3470e-01, 4.9103e-02, 7.8420e-01,\n",
            "         5.8720e-01, 1.2671e-01, 8.0031e-01, 5.2237e-01, 4.8292e-01, 7.5590e-01,\n",
            "         4.7389e-01, 5.8536e-01, 9.4485e-01, 7.5220e-01, 4.9507e-01, 5.1940e-01,\n",
            "         4.5368e-02, 7.6655e-01, 8.9962e-02, 2.1974e-01, 7.7944e-01, 9.5008e-01,\n",
            "         5.2931e-02, 3.9477e-01, 1.8688e-01, 4.6288e-01, 8.6731e-01, 9.5707e-02,\n",
            "         4.3523e-01, 2.2886e-01, 2.2480e-02, 3.4116e-01, 5.8843e-01, 2.8891e-01,\n",
            "         6.7269e-01, 7.8901e-01, 8.3074e-01, 8.0720e-01, 7.9972e-01, 6.2766e-01,\n",
            "         5.8966e-01, 4.7921e-01, 8.9077e-01, 8.8568e-01, 3.3668e-01, 2.8350e-01,\n",
            "         3.7946e-01, 2.1869e-01, 2.6210e-01, 6.1805e-01, 2.8067e-01, 3.4879e-01,\n",
            "         3.8360e-01, 4.8669e-01, 1.8745e-01, 5.2823e-01, 7.7946e-01, 6.8040e-01,\n",
            "         9.6582e-01, 3.8303e-01, 7.5984e-01, 4.5345e-01, 3.5986e-01, 4.2814e-01,\n",
            "         8.1103e-01, 4.1428e-01, 6.9601e-01, 4.8625e-01, 1.0720e-01, 2.7930e-01,\n",
            "         5.8660e-01, 1.9926e-02, 9.6731e-01, 9.1077e-01, 5.0905e-01, 9.8848e-01,\n",
            "         4.0954e-02, 6.3378e-01, 5.5683e-01, 6.9598e-01, 6.7916e-01, 1.9390e-01,\n",
            "         8.2906e-01, 6.0722e-01, 2.1907e-01, 8.3570e-01, 6.5513e-02, 4.4651e-03,\n",
            "         1.8463e-01, 4.9470e-01, 2.3591e-02, 7.3250e-01, 9.1957e-01, 4.7280e-01,\n",
            "         4.8801e-01, 2.0918e-01, 1.0886e-01, 1.5641e-01, 7.6768e-01, 1.9960e-01,\n",
            "         9.3986e-01, 3.8187e-01, 1.5912e-01, 7.6933e-01, 1.6835e-01, 8.4409e-01,\n",
            "         4.3238e-01, 2.7157e-01, 9.2480e-02, 9.4792e-01, 1.4152e-02, 7.9656e-01,\n",
            "         4.8916e-02, 5.5200e-01, 9.2877e-01, 1.3314e-01, 3.4518e-01, 2.8525e-01,\n",
            "         3.6255e-02, 6.7657e-01, 8.6397e-04, 2.7236e-01, 9.4321e-01, 9.1401e-01,\n",
            "         6.2949e-01, 3.6858e-01, 6.0128e-01, 3.8303e-01, 2.9979e-01, 5.7682e-01,\n",
            "         3.1200e-01, 5.6519e-01, 1.4693e-01, 1.2710e-02, 4.3782e-01, 8.3512e-01,\n",
            "         3.1555e-01, 7.6123e-01, 8.6239e-01, 6.4096e-01, 7.8362e-01, 4.5349e-01,\n",
            "         5.9339e-01, 5.5540e-01, 3.9885e-02, 3.7789e-01, 7.7069e-01, 5.4475e-03,\n",
            "         3.0118e-01, 6.7749e-01, 8.6712e-01, 3.5499e-01, 5.4710e-01, 5.4584e-01,\n",
            "         9.0239e-01, 7.0860e-01, 4.4216e-01, 6.8184e-02, 2.9795e-02, 9.4044e-01,\n",
            "         2.6007e-01, 8.6736e-01, 7.3379e-01, 4.8094e-01, 3.7694e-01, 6.7411e-01,\n",
            "         3.5461e-01, 3.2711e-01, 8.2099e-01, 6.4302e-01, 3.8545e-01, 4.4477e-01,\n",
            "         6.8840e-02, 9.9537e-01, 1.3581e-01, 4.5843e-01, 5.0917e-01, 8.8485e-01,\n",
            "         4.2610e-01, 2.1363e-01, 4.9694e-01, 9.8769e-01, 7.1514e-01, 3.4580e-01,\n",
            "         3.1887e-01, 1.2912e-01, 2.1583e-01, 9.1653e-01, 1.2751e-01, 2.1147e-01,\n",
            "         6.4208e-02, 2.8444e-01, 3.9964e-01, 2.9449e-01, 3.3846e-01, 7.5736e-01,\n",
            "         9.7706e-01, 5.8365e-01, 6.3257e-01, 6.8568e-01, 8.9810e-01, 8.2701e-02,\n",
            "         1.4180e-01, 8.6884e-01, 5.5579e-01, 3.3896e-02, 2.7359e-01, 8.1974e-01,\n",
            "         8.3220e-02, 5.0362e-01, 8.8378e-01, 3.4895e-01, 9.8216e-01, 6.0701e-01,\n",
            "         9.4903e-01, 2.0489e-01, 2.4970e-01, 8.5136e-01, 2.9381e-01, 6.9578e-01,\n",
            "         3.6808e-02, 5.9300e-01, 2.5923e-01, 7.3373e-01, 1.8709e-01, 5.8776e-01,\n",
            "         9.3489e-01, 3.5250e-01, 2.9000e-01, 5.9355e-02, 3.0703e-01, 5.3732e-01,\n",
            "         4.0409e-01, 3.3882e-01, 2.6015e-01, 9.2694e-01, 6.2741e-01, 4.8887e-01,\n",
            "         2.9156e-02, 1.7993e-01, 4.0282e-02, 2.4392e-01, 1.5548e-01, 8.2624e-01,\n",
            "         5.3473e-01, 2.0546e-02, 4.9388e-01, 8.8159e-01, 3.5475e-01, 6.2108e-01,\n",
            "         2.4200e-01, 5.4292e-01, 9.0582e-01, 8.5885e-01, 3.7261e-01, 2.4570e-01,\n",
            "         6.1193e-01, 5.7184e-01, 5.4636e-01, 9.8947e-01, 1.5086e-01, 2.2637e-01,\n",
            "         5.0402e-01, 5.2185e-01, 3.2429e-01, 7.9622e-01, 2.7712e-01, 9.7940e-01,\n",
            "         7.3443e-01, 3.7027e-01, 2.9838e-01, 4.9142e-01, 4.8374e-01, 4.6165e-01,\n",
            "         5.1528e-01, 7.8103e-01, 1.3446e-01, 2.4345e-01, 2.7850e-01, 2.1444e-02,\n",
            "         5.6038e-01, 1.1507e-01, 6.8466e-02, 2.5652e-02, 6.9471e-01, 9.3370e-01,\n",
            "         9.2631e-01, 2.5174e-01, 5.6026e-01, 3.6026e-01, 6.6806e-01, 4.7684e-01,\n",
            "         6.7861e-01, 7.9316e-01, 9.4557e-01, 6.6101e-02, 4.6765e-01, 4.6133e-02,\n",
            "         4.3266e-01, 7.4247e-01, 9.0118e-01, 1.1125e-01, 3.6535e-01, 4.9412e-01,\n",
            "         1.6257e-01, 7.1251e-01, 6.3672e-01, 2.8387e-01, 3.0163e-01, 7.3245e-01,\n",
            "         2.6632e-01, 6.4294e-02, 1.8160e-01, 5.4132e-01, 1.5629e-01, 5.8212e-01,\n",
            "         4.0517e-01, 7.4578e-01, 1.2294e-01, 7.5084e-01, 4.1115e-01, 2.6350e-01,\n",
            "         4.6945e-01, 1.0059e-01, 3.9238e-02, 4.4415e-01, 4.0859e-01, 1.1570e-02,\n",
            "         1.2017e-01, 1.8838e-01, 6.1098e-01, 3.4714e-02, 8.4112e-01, 3.4443e-03,\n",
            "         1.3180e-01, 3.6062e-01, 8.0306e-01, 2.6026e-01, 1.1650e-01, 6.2045e-01,\n",
            "         8.0295e-01, 6.7832e-01, 6.5546e-01, 4.9213e-01, 5.5931e-01, 4.1140e-01,\n",
            "         6.3603e-01, 6.0322e-01, 8.8502e-01, 7.4533e-02, 8.2028e-01, 7.8289e-01,\n",
            "         5.9598e-01, 1.6360e-01, 9.4883e-01, 4.2479e-01, 2.6892e-01, 6.0257e-01,\n",
            "         8.0099e-01, 4.0078e-01, 7.5232e-01, 5.4935e-01, 1.2127e-01, 6.1953e-01,\n",
            "         4.2423e-01, 8.3637e-01, 9.3327e-01, 7.4015e-01, 3.7054e-01, 4.0309e-01,\n",
            "         7.4732e-01, 8.2817e-01, 6.4328e-01, 8.8699e-01, 5.9715e-03, 9.4021e-01,\n",
            "         3.5473e-01, 8.1178e-01, 2.2763e-01, 3.4800e-02, 5.2102e-02, 4.1872e-01,\n",
            "         4.1166e-01, 1.9230e-01, 2.6807e-01, 6.2587e-01, 1.8999e-01, 7.4657e-01,\n",
            "         8.6190e-01, 9.3637e-01, 8.9302e-01, 4.3445e-01, 3.5347e-01, 6.9123e-01,\n",
            "         8.2747e-01, 3.2475e-01, 9.1884e-01, 5.4046e-01, 5.3068e-02, 8.1088e-01,\n",
            "         6.6405e-01, 3.3913e-03, 4.9326e-02, 3.9995e-01, 4.7640e-01, 7.0970e-02,\n",
            "         6.3112e-01, 9.0363e-02, 3.3110e-01, 2.0567e-01, 7.3589e-01, 8.9579e-01,\n",
            "         2.6556e-01, 2.3282e-01, 8.8327e-01, 3.6352e-01, 5.2920e-01, 9.7920e-01,\n",
            "         7.2056e-01, 7.1584e-01, 8.7391e-01, 3.8346e-01, 6.6143e-02, 5.3053e-01,\n",
            "         5.1250e-01, 5.8736e-01, 2.6622e-01, 6.2655e-01, 1.7668e-01, 4.3834e-01,\n",
            "         6.2729e-01, 2.1042e-01, 7.0683e-01, 1.8921e-01, 3.5854e-01, 1.1047e-02,\n",
            "         4.9750e-01, 7.6596e-01, 3.9334e-02, 4.0444e-01, 2.1821e-01, 4.6914e-01,\n",
            "         9.3628e-01, 3.2896e-01, 4.8102e-01, 1.7506e-01, 1.6929e-01, 9.0207e-01,\n",
            "         3.7603e-01, 9.5514e-01, 7.7597e-01, 1.8890e-01, 2.5598e-01, 6.0674e-01,\n",
            "         3.0527e-01, 3.8747e-01, 1.2572e-01, 7.7191e-02, 1.1214e-01, 2.9552e-01,\n",
            "         5.3808e-01, 6.4745e-01, 6.1853e-01, 2.2691e-01, 3.4650e-02, 9.4342e-01,\n",
            "         3.8739e-01, 3.0152e-01, 2.3345e-01, 9.7693e-01, 4.3616e-01, 2.3579e-01,\n",
            "         8.8120e-01, 2.0955e-01, 2.7781e-01, 2.9038e-01, 5.9139e-01, 9.0734e-01,\n",
            "         1.9511e-01, 6.9578e-02, 9.4853e-02, 6.0402e-01, 5.1117e-01, 8.4771e-01,\n",
            "         3.6606e-01, 9.1614e-01, 5.0151e-01, 7.3102e-01, 5.9551e-01, 6.1179e-01,\n",
            "         4.2015e-02, 1.2924e-02, 2.3436e-01, 2.5481e-01, 8.0342e-01, 2.8836e-01,\n",
            "         5.6731e-01, 2.3685e-01, 4.2015e-01, 1.9774e-01, 8.5126e-01, 9.9945e-01,\n",
            "         2.0040e-01, 1.8495e-01, 9.4435e-01, 3.8937e-01, 5.3898e-01, 6.9084e-01,\n",
            "         4.3045e-01, 6.6288e-01, 2.9434e-01, 5.5954e-02, 1.5206e-01, 7.2261e-01,\n",
            "         8.7444e-01, 3.8851e-01, 6.5245e-01, 3.6079e-01, 6.8822e-01, 7.1670e-01,\n",
            "         6.6814e-01, 9.7717e-01, 6.9979e-01, 1.6493e-01, 9.2366e-01, 6.3414e-01,\n",
            "         4.8153e-03, 4.6662e-01, 3.2011e-01, 5.3753e-01, 7.2455e-01, 7.3778e-02,\n",
            "         5.4289e-01, 3.1367e-01, 9.9372e-01, 9.7805e-02, 1.4233e-01, 7.0383e-01,\n",
            "         9.2355e-01, 6.2670e-02, 5.4038e-01, 8.6310e-01, 1.2620e-01, 9.6617e-01,\n",
            "         4.5045e-01, 6.2790e-01, 1.0539e-02, 9.8332e-01, 5.4596e-01, 1.3144e-01,\n",
            "         5.5068e-02, 8.2063e-01, 8.9742e-01, 8.3944e-01, 6.6711e-01, 6.7612e-01,\n",
            "         9.7936e-01, 6.0691e-01, 2.6986e-01, 7.6929e-01, 1.7064e-02, 8.6060e-01,\n",
            "         4.9450e-01, 2.5832e-01, 5.1553e-01, 2.2180e-01, 3.9843e-01, 6.9008e-01,\n",
            "         8.5709e-01, 1.7262e-01, 1.2518e-01, 8.1418e-01, 9.0514e-01, 5.9434e-01,\n",
            "         9.1985e-01, 2.8396e-01, 7.6231e-01, 2.5666e-01, 3.2878e-01, 9.2209e-01,\n",
            "         1.0114e-01, 3.9013e-01, 8.5907e-01, 3.9091e-01, 5.2296e-01, 8.0425e-01,\n",
            "         1.4116e-01, 7.4947e-01, 2.7563e-01, 9.7006e-01, 4.6066e-01, 7.4121e-01,\n",
            "         9.1464e-01, 1.5983e-01, 5.4867e-01, 7.2538e-01, 5.9653e-01, 1.4706e-02,\n",
            "         9.9823e-02, 6.8303e-01, 7.4414e-01, 7.2302e-01, 4.3171e-01, 3.6302e-01,\n",
            "         8.9488e-01, 3.9434e-02, 5.0050e-01, 1.3283e-01, 3.6569e-01, 4.5967e-01,\n",
            "         7.2063e-01, 9.3748e-01, 3.4663e-01, 1.7030e-02, 3.2499e-01, 3.4973e-01,\n",
            "         9.6832e-02, 7.0381e-01, 5.8067e-01, 2.7180e-01, 5.9392e-01, 7.6709e-01,\n",
            "         4.7825e-01, 5.5629e-01, 3.3727e-02, 1.9087e-01, 7.6986e-01, 9.8939e-01,\n",
            "         1.8054e-02, 3.8509e-01, 2.5309e-01, 5.0595e-01, 8.3193e-01, 2.7173e-01,\n",
            "         5.6451e-02, 4.3299e-01, 2.4563e-01, 1.9468e-01, 6.2596e-01, 2.7419e-01,\n",
            "         1.5021e-01, 6.1970e-01, 1.3706e-01, 2.8286e-01, 8.6291e-01, 3.5933e-01,\n",
            "         5.0050e-01, 8.3949e-01, 1.1344e-01, 5.8654e-01, 1.8998e-01, 2.9114e-01,\n",
            "         6.5031e-01, 6.6420e-01, 8.9553e-01, 9.1221e-01, 1.4593e-01, 9.7058e-01,\n",
            "         9.4155e-01, 7.2661e-01, 8.3362e-01, 8.6915e-01, 5.7851e-01, 7.2687e-01,\n",
            "         1.9090e-01, 6.7065e-01, 3.1632e-01, 7.9912e-01, 1.3018e-01, 1.0418e-01,\n",
            "         6.6417e-01, 1.8177e-01, 1.3902e-01, 7.8425e-01, 5.0300e-01, 7.9582e-01,\n",
            "         2.8919e-01, 4.2620e-01, 7.3300e-01, 6.6780e-01, 6.5997e-02, 6.4569e-01,\n",
            "         1.5958e-01, 2.6353e-01, 4.3993e-01, 6.5467e-02, 4.0732e-01, 1.3892e-01,\n",
            "         4.0996e-01, 2.1320e-01, 3.5028e-01, 1.7762e-02, 1.4838e-01, 3.0204e-01,\n",
            "         9.1696e-01, 9.0418e-01, 5.3870e-01, 1.2457e-01, 6.7489e-01, 1.9000e-01,\n",
            "         7.0749e-01, 7.7586e-01, 1.7022e-01, 8.0744e-01, 3.8048e-01, 9.6613e-01,\n",
            "         3.4745e-01, 7.4172e-01, 9.6774e-01, 5.8794e-01, 4.2012e-01, 7.5996e-01,\n",
            "         8.3832e-02, 8.0592e-01, 2.7572e-01, 4.9608e-01, 9.3867e-01, 3.9057e-01,\n",
            "         5.7133e-01, 9.6406e-01, 3.9671e-01, 8.0869e-01, 2.4601e-01, 5.7323e-01,\n",
            "         7.0101e-01, 2.1906e-01, 1.3646e-01, 6.9014e-01, 3.0080e-01, 5.9806e-01,\n",
            "         1.7241e-01, 5.8552e-02, 4.8796e-01, 8.9466e-01, 1.6409e-01, 1.0172e-01,\n",
            "         7.0553e-01, 2.2196e-01, 3.7222e-01, 9.0630e-01, 4.1143e-01, 7.4948e-01,\n",
            "         2.8017e-01, 2.5844e-01, 4.3619e-01, 7.7488e-02, 3.8633e-01, 8.9305e-01,\n",
            "         3.8368e-01, 7.9530e-01, 7.9313e-01, 9.4983e-01, 8.6568e-01, 3.4074e-01,\n",
            "         1.9989e-01, 6.7969e-01, 3.7731e-01, 4.0343e-01, 4.6830e-01, 2.4136e-01,\n",
            "         3.8860e-01, 5.2389e-01, 6.4110e-03, 6.4225e-02, 1.1879e-01, 3.2402e-01,\n",
            "         5.4502e-01, 4.0833e-01, 8.8988e-01, 5.5756e-01]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCSy_xaiIv8S"
      },
      "source": [
        "Next, we run the input data through the model through each of its layers\n",
        "to make a prediction. This is the **forward pass**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JCP1OoAXIv8S"
      },
      "outputs": [],
      "source": [
        "prediction = model(data) # forward pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction)"
      ],
      "metadata": {
        "id": "vw3f_bIJLxdp",
        "outputId": "d88a202b-d0d5-4452-d8fe-157bc69d49ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-7.1721e-01, -8.0317e-01, -4.6254e-01, -1.3953e+00, -6.5891e-01,\n",
            "         -3.8245e-02, -3.0617e-01,  2.9008e-01,  1.9753e-01, -7.7796e-01,\n",
            "         -9.2397e-01, -8.4715e-01, -1.1115e-01, -5.9057e-01, -1.1899e+00,\n",
            "         -3.2618e-01, -6.3384e-01, -1.2378e-01, -4.2011e-01, -1.0706e-01,\n",
            "         -1.4644e+00, -5.8291e-01, -1.2522e+00,  1.4706e-01, -6.2728e-01,\n",
            "         -1.3277e+00, -9.5223e-01, -1.1591e+00, -1.0244e+00, -3.7779e-01,\n",
            "         -1.1520e+00, -8.0115e-01, -6.4117e-01, -6.1042e-01, -3.7982e-01,\n",
            "         -3.6364e-01,  5.6635e-01, -8.1175e-01, -4.4226e-01,  2.2278e-01,\n",
            "         -4.6674e-01, -6.3391e-01, -1.0514e+00, -2.3385e-01, -4.1905e-01,\n",
            "         -7.0135e-01, -6.8939e-01, -2.8507e-01, -1.2978e+00, -1.2454e+00,\n",
            "         -6.3175e-01,  5.8784e-01, -2.6600e-01, -6.7488e-01, -2.3692e-01,\n",
            "         -9.4356e-01, -4.2503e-01, -1.5013e+00, -9.5428e-01, -2.2992e-01,\n",
            "          6.6360e-01,  1.0867e-01, -2.8330e-01,  2.7000e-02, -5.8279e-01,\n",
            "         -5.0296e-01, -4.3025e-01, -4.3698e-01, -8.3060e-01, -7.4545e-01,\n",
            "         -1.3376e+00,  1.3406e-02, -1.3601e+00, -2.3163e-01, -1.1846e+00,\n",
            "         -1.4093e+00,  3.2946e-01, -5.1782e-01,  2.6025e-01,  3.7139e-02,\n",
            "         -1.0697e+00, -1.4814e+00,  2.5533e-03, -5.8517e-01, -6.1948e-01,\n",
            "         -1.1696e-01,  1.2267e-01,  7.4293e-01,  4.9947e-03, -7.7492e-01,\n",
            "         -1.4307e+00, -1.0701e+00, -1.9039e+00, -3.7461e-01,  6.2104e-01,\n",
            "         -1.9329e+00, -4.5457e-01, -5.3253e-01, -1.4222e+00, -4.7580e-02,\n",
            "         -1.3258e+00, -7.7881e-01, -9.1687e-01, -3.4290e-01, -2.3756e-01,\n",
            "         -6.6604e-01, -5.3624e-01, -1.4209e+00, -1.3364e+00, -1.6882e+00,\n",
            "         -1.2645e+00, -5.7530e-01,  1.3125e+00,  3.0626e-01,  1.2770e-01,\n",
            "         -1.1855e+00, -8.8742e-01, -3.5413e-01,  3.6870e-01, -4.7995e-01,\n",
            "         -9.0239e-01,  6.1808e-02,  2.6169e-01,  3.0338e-01,  8.4099e-01,\n",
            "         -1.7094e-01,  8.9090e-02, -1.3628e+00, -1.3690e+00, -9.5033e-01,\n",
            "         -1.3177e+00, -1.5755e+00, -1.0289e+00, -1.3969e+00, -8.5751e-01,\n",
            "         -1.4688e+00, -1.0453e+00, -9.6864e-01, -1.2129e+00, -1.4682e+00,\n",
            "         -1.3619e+00, -1.6417e+00, -2.0675e+00, -1.7797e+00, -4.8088e-01,\n",
            "         -4.1845e-01, -7.5495e-01, -1.7017e+00, -1.0981e+00, -1.2296e+00,\n",
            "          4.8191e-01,  1.5215e+00, -8.2104e-01, -3.7613e-01,  3.1488e-02,\n",
            "          3.2897e-01, -4.1311e-01,  8.1522e-02,  3.3699e-01, -6.0067e-02,\n",
            "          5.6123e-01,  3.0908e-01, -3.9174e-02,  3.9705e-01,  6.0538e-02,\n",
            "         -2.2200e-01, -3.9873e-01, -4.7349e-01,  4.2482e-01, -2.5826e-01,\n",
            "         -2.7834e-02,  8.9790e-01,  4.4648e-01,  2.1047e-01, -3.2742e-02,\n",
            "         -5.7173e-01,  2.8537e-01,  1.5692e-01,  7.3194e-01,  2.2137e-01,\n",
            "          3.1983e-01,  3.1262e-01,  3.9995e-01,  1.2772e-01,  5.9257e-01,\n",
            "          5.0528e-01,  6.0358e-01,  8.4528e-02, -2.1672e-02,  2.9206e-01,\n",
            "         -2.9078e-01,  1.5537e-01,  2.3658e-01,  3.7615e-01, -5.4519e-01,\n",
            "          6.6708e-01,  7.1929e-02,  2.4884e-01,  3.0846e-01,  4.8475e-01,\n",
            "          1.6712e-01, -2.1540e-03,  3.3656e-01,  5.4175e-01,  3.2701e-01,\n",
            "          4.2141e-01,  2.6543e-02,  5.0497e-01,  9.9346e-01,  3.0746e-01,\n",
            "         -2.3376e-01,  4.8112e-01,  5.7411e-01, -3.3959e-02,  1.3227e-01,\n",
            "          2.0828e-01,  7.5311e-02,  2.6665e-01, -4.9550e-01,  7.2112e-01,\n",
            "          3.0761e-01, -7.2641e-02, -8.0154e-02,  8.3849e-01,  3.0236e-01,\n",
            "          1.2940e-01,  9.7286e-03,  4.4606e-01, -4.5823e-01, -2.0936e-01,\n",
            "         -1.7112e-01,  3.9212e-01,  3.3928e-01, -3.1881e-01,  4.2472e-01,\n",
            "          2.1238e-01,  1.9399e-01,  2.8425e-01,  7.6297e-01, -3.0210e-02,\n",
            "          4.3973e-01, -1.4891e-01,  3.7161e-01,  2.7183e-01, -3.2570e-01,\n",
            "          2.6197e-01,  4.7977e-01, -1.9205e-01,  5.0111e-01, -1.8577e-02,\n",
            "          2.3229e-01,  6.9717e-01, -5.7939e-01,  5.9348e-01,  6.9390e-01,\n",
            "         -5.8807e-01,  3.9362e-01,  3.7532e-01, -1.1973e-01,  2.4539e-01,\n",
            "         -5.4203e-01, -8.5956e-01, -1.3914e-01,  1.3037e-01,  4.2480e-01,\n",
            "          8.5326e-01,  4.5235e-01,  6.4021e-01,  1.1080e-01, -6.0336e-01,\n",
            "         -8.3402e-01, -1.0662e+00, -5.6146e-01,  3.6798e-01, -1.3696e+00,\n",
            "         -1.2075e+00, -1.1943e+00, -8.5291e-01, -1.2624e+00, -5.7159e-01,\n",
            "         -5.6703e-01,  8.7995e-01,  7.3930e-01,  1.4312e-01,  4.8250e-01,\n",
            "          9.8113e-01, -1.9655e-01, -2.0219e-01, -6.6499e-01, -1.5266e+00,\n",
            "         -1.0891e+00, -1.2793e+00, -5.7710e-01, -1.1249e+00, -1.0946e+00,\n",
            "         -7.8368e-01, -8.6510e-01, -1.2467e+00, -3.7015e-01, -1.6415e-01,\n",
            "         -1.5534e+00, -1.0156e+00, -3.1732e-01, -3.1775e-01, -1.1053e+00,\n",
            "         -8.6865e-01,  2.2914e-01, -8.0488e-01, -1.4061e+00, -6.0004e-01,\n",
            "          4.2725e-01,  4.9114e-03, -4.7166e-02,  5.9112e-01,  6.0248e-01,\n",
            "         -1.6222e-01, -1.0667e+00, -1.1628e+00, -1.0993e+00, -7.3925e-01,\n",
            "         -1.6171e+00, -1.2259e+00, -1.1290e+00, -1.5503e+00, -1.3029e+00,\n",
            "         -1.5033e+00, -1.1666e+00,  1.3982e-01, -2.0315e-01, -7.9566e-01,\n",
            "         -1.6443e-02, -1.3824e-01,  1.1787e-01,  4.6271e-01, -5.0329e-01,\n",
            "         -8.9546e-01, -1.6069e+00, -1.4231e-01,  9.6729e-01, -1.1492e+00,\n",
            "         -3.9882e-01,  7.9053e-01, -4.5688e-01, -1.1876e+00, -8.7123e-01,\n",
            "          6.3617e-01, -8.0848e-01, -1.7759e+00, -1.0225e-01, -1.0648e+00,\n",
            "         -1.2034e+00, -2.0962e+00, -1.1667e+00, -5.9443e-01, -7.7430e-01,\n",
            "          4.2719e-01,  1.3126e+00, -1.2663e-01,  5.7710e-01,  6.8488e-01,\n",
            "         -2.0932e-01,  5.3763e-01,  1.6626e-01,  2.1821e-01, -3.2478e-01,\n",
            "         -4.8265e-01, -1.2492e+00, -4.2831e-01, -7.5783e-01, -5.0200e-01,\n",
            "         -4.9803e-01, -2.4228e-01, -3.3014e-01, -4.4278e-02, -1.5835e-01,\n",
            "         -9.1260e-01, -1.2461e+00,  5.3563e-01, -2.0377e-01, -4.9296e-01,\n",
            "          3.9949e-01, -4.5873e-01, -3.3180e-01, -7.4441e-01, -9.2830e-01,\n",
            "         -3.2952e-01, -9.5269e-01, -1.0678e+00, -9.7203e-01, -4.4185e-01,\n",
            "          4.9092e-01,  1.0197e-01, -1.5971e+00, -2.0030e+00, -1.5871e-01,\n",
            "          4.3706e-01, -1.2629e+00, -9.7662e-01,  2.4868e-01,  1.5364e-01,\n",
            "         -7.2457e-01,  7.8325e-01,  3.5841e-01, -2.0290e+00, -2.0655e+00,\n",
            "         -6.9015e-01, -3.0524e-01, -3.0771e-01, -4.7838e-01,  1.2202e+00,\n",
            "         -2.5001e-01,  4.0825e-01,  1.7471e+00,  8.1159e-01,  3.5066e-01,\n",
            "          4.9028e-01, -2.4628e-01,  4.6460e-02,  3.1519e-01,  1.1310e+00,\n",
            "          1.1115e+00,  1.2475e+00, -1.0025e-01,  2.3281e-01,  1.1280e-01,\n",
            "         -8.4216e-01,  1.2704e-01,  1.2732e+00,  1.6627e+00,  5.0090e-01,\n",
            "         -9.4679e-01,  6.3911e-02,  2.9555e-01,  6.5021e-01,  6.0913e-01,\n",
            "          1.3255e+00, -2.4814e-01, -4.9219e-01,  5.2746e-01,  6.9924e-01,\n",
            "          7.4018e-01,  4.6521e-01,  2.6942e-01, -5.3630e-01, -2.6764e-01,\n",
            "          3.3506e-01,  3.9359e-01,  1.6782e+00,  1.0318e+00, -7.4349e-01,\n",
            "         -4.7808e-01,  9.8931e-01,  7.1464e-01,  1.2522e-01, -8.5581e-02,\n",
            "          5.0285e-01,  1.8578e+00,  1.3798e+00, -3.6776e-01,  1.0871e+00,\n",
            "         -1.1395e+00,  9.8366e-01,  9.9203e-01,  2.4871e+00,  1.0861e+00,\n",
            "         -4.1680e-01, -1.1520e+00, -9.3148e-02,  2.1697e-02,  1.7204e+00,\n",
            "          9.4698e-01,  3.9974e-01,  1.6404e-01,  1.0631e+00,  2.5777e-03,\n",
            "          3.9265e-01,  1.1206e-01,  6.0981e-01,  4.6515e-01,  5.4955e-01,\n",
            "         -1.6991e-01,  3.8536e-01,  3.7891e-01, -6.6440e-01, -1.6604e+00,\n",
            "         -2.1465e-02,  1.1276e-01,  9.3682e-01,  1.6899e+00,  1.0464e+00,\n",
            "          7.9456e-01,  7.0755e-01,  9.3549e-01, -9.4712e-01,  1.6921e+00,\n",
            "         -8.3614e-01,  9.3239e-02, -4.0208e-01, -3.4093e-01,  1.6699e+00,\n",
            "         -1.7053e+00,  7.8602e-01,  1.2966e+00,  8.1160e-01,  8.9917e-01,\n",
            "          1.2892e+00,  9.7651e-01,  7.1261e-01,  3.8180e-01, -1.3742e-02,\n",
            "         -1.2808e+00, -9.1526e-01,  1.1315e+00,  4.2591e-01,  1.0754e+00,\n",
            "          1.7983e+00,  6.3676e-01, -6.0603e-02,  1.4152e+00,  3.7634e-01,\n",
            "         -7.6146e-01,  1.8377e-01,  8.1891e-01,  1.5271e+00,  5.8603e-01,\n",
            "         -6.1260e-01, -1.2914e-01, -5.9400e-01,  5.1390e-01,  1.1862e-01,\n",
            "          7.3386e-01,  3.9030e-01,  7.3193e-02, -1.0340e+00,  3.2419e-01,\n",
            "         -1.4118e-01, -4.9012e-01, -9.7298e-01,  5.3079e-02,  1.0987e+00,\n",
            "         -1.2107e+00,  1.9064e+00,  1.2453e+00,  9.7362e-01,  3.0466e-01,\n",
            "          9.2666e-01,  8.0717e-01, -1.8167e+00, -1.2373e+00,  5.7782e-02,\n",
            "         -3.9348e-01,  5.0307e-01,  6.7870e-01, -1.9540e-01, -1.5678e+00,\n",
            "         -6.5919e-01,  3.4011e-01, -1.4228e-01,  1.0978e+00,  9.5746e-01,\n",
            "          2.9199e-01, -3.2354e-01,  6.7396e-01,  2.8047e-01, -9.7571e-01,\n",
            "         -9.9597e-01,  3.4962e-01,  1.1001e+00,  4.9692e-01, -5.4733e-01,\n",
            "          1.1269e+00,  5.9827e-02,  9.5654e-01, -7.7726e-01,  3.4904e-01,\n",
            "         -3.0751e-01, -1.2022e+00,  1.2363e+00,  6.6190e-01,  2.8822e-01,\n",
            "          9.5927e-02, -1.5956e-01,  4.4979e-01,  9.9662e-01,  1.2239e+00,\n",
            "          8.6312e-01, -4.9878e-01,  1.8755e+00,  1.1316e+00,  1.3565e+00,\n",
            "         -6.6045e-01,  5.5486e-01, -3.5277e-01,  1.0726e+00,  5.5076e-01,\n",
            "         -7.8919e-01,  1.3161e+00,  1.7391e-01, -6.6301e-01,  7.0638e-01,\n",
            "          2.3930e+00,  2.2190e-02, -1.2435e-01, -8.1167e-01,  6.1929e-01,\n",
            "          2.5110e-01,  1.3983e+00, -5.2364e-01,  5.4479e-01, -3.6246e-01,\n",
            "          1.0277e+00,  7.2490e-01, -3.5876e-01,  8.2937e-01,  1.9548e-01,\n",
            "          3.6056e-01,  1.2378e+00,  4.7230e-01,  2.0949e+00,  9.1565e-01,\n",
            "          8.4136e-01,  4.3485e-01,  2.3998e-01,  6.0772e-01,  1.0677e-01,\n",
            "         -1.1710e+00,  8.3535e-01, -3.8557e-01, -1.1962e+00,  1.5377e-01,\n",
            "          3.2835e-02,  9.0848e-01,  4.8941e-01,  1.5190e+00, -3.9671e-01,\n",
            "          4.3147e-01,  1.4584e+00,  9.3749e-01,  9.7121e-01,  5.1151e-01,\n",
            "         -1.8401e+00,  1.3684e+00, -2.0645e-02,  1.5456e+00,  6.9798e-01,\n",
            "         -1.0571e+00,  6.6660e-01,  5.2787e-01, -4.3679e-01, -1.3673e+00,\n",
            "          9.7636e-01,  3.6877e-02,  8.5370e-01,  9.2257e-01,  2.4639e-02,\n",
            "          8.4351e-01, -1.4560e-01, -2.0583e-01,  9.7248e-02,  6.3645e-01,\n",
            "         -2.5530e-01, -1.0681e+00, -1.9236e-01, -6.0974e-01,  5.7180e-01,\n",
            "          1.2439e-01,  1.3896e+00,  5.8365e-01, -9.2467e-01, -6.3082e-01,\n",
            "          1.5073e-01, -2.9263e-01, -1.7893e-01,  6.4743e-01,  1.5956e+00,\n",
            "         -9.4780e-01,  1.4931e+00,  1.1757e+00,  1.0191e+00,  3.5173e-01,\n",
            "          6.4447e-01,  3.3962e-01, -6.9569e-01,  3.5336e-01,  1.3132e+00,\n",
            "         -1.4168e+00, -3.5471e-01, -1.0854e+00, -2.1158e-01, -5.5326e-01,\n",
            "         -7.0558e-01,  9.9104e-01,  7.3879e-01,  5.2529e-01, -8.2191e-01,\n",
            "          1.1256e+00,  1.8179e+00,  1.9598e-02, -4.1283e-01,  6.4563e-01,\n",
            "          1.8704e+00, -5.0844e-01, -1.3874e-01,  3.2216e-01,  7.2401e-01,\n",
            "         -1.9256e-01, -5.5565e-01,  3.4133e-01,  7.8800e-01,  4.8168e-01,\n",
            "          1.3156e+00,  8.6032e-01,  2.0253e-01, -4.2946e-01,  8.0528e-01,\n",
            "         -3.7397e-01,  5.8129e-01, -5.0972e-01, -4.5768e-01,  7.5173e-01,\n",
            "          3.2727e-01,  3.7990e-01,  1.5056e+00,  3.4929e-01, -5.5478e-01,\n",
            "          1.4850e+00, -6.2602e-01, -1.2223e-01,  1.3876e+00, -7.3446e-02,\n",
            "          1.7286e-01,  1.9739e+00, -8.9147e-01,  1.6264e+00, -1.6348e+00,\n",
            "          1.8953e-01, -6.0609e-02,  6.9253e-01,  8.2947e-01,  5.1439e-01,\n",
            "          1.1659e+00,  1.2577e-01,  3.9317e-01,  6.2285e-01,  4.4892e-01,\n",
            "          2.5836e-02, -2.6162e-01,  8.3194e-01,  9.9906e-01,  1.2891e+00,\n",
            "          5.7985e-01, -1.7512e-01,  1.6856e-01,  2.6292e-01,  7.5316e-01,\n",
            "         -4.6227e-01,  1.0302e+00,  3.0954e-02,  1.5445e+00, -9.1456e-02,\n",
            "          2.3635e-02,  6.1542e-01,  5.3412e-01,  9.0719e-01,  1.4132e+00,\n",
            "          9.2868e-01,  3.3502e-01,  4.4196e-01,  2.4654e-01,  1.4738e+00,\n",
            "          1.4258e-01,  3.2187e-01,  1.7651e+00,  8.0753e-01,  1.0482e+00,\n",
            "          4.7370e-01,  3.4099e-01,  8.6046e-01,  1.3436e+00, -7.1575e-01,\n",
            "         -9.3281e-01, -9.8239e-01,  8.2134e-01,  9.2244e-01,  1.4595e+00,\n",
            "          3.2032e-01,  9.6010e-01,  1.3417e+00,  1.6646e-01,  7.3716e-02,\n",
            "          5.7198e-01,  8.2962e-01,  1.5471e+00,  6.8015e-01,  1.5949e-01,\n",
            "         -1.0654e-01,  7.4171e-01,  5.7458e-01, -5.1776e-01,  3.0743e-01,\n",
            "         -8.5204e-01, -6.3217e-02, -1.3180e+00, -1.2087e+00,  1.0169e+00,\n",
            "          1.0816e+00,  3.7944e-01,  3.7249e-02,  1.5856e+00,  2.2403e-02,\n",
            "         -6.0410e-01,  8.7786e-01, -1.4338e-01,  1.6731e+00, -1.1936e+00,\n",
            "         -1.3709e-01,  4.4219e-01, -1.1065e+00,  1.7326e+00,  5.0925e-01,\n",
            "         -1.6678e+00, -6.6003e-01,  3.9908e-01,  9.9172e-01,  1.0511e+00,\n",
            "         -1.3234e+00,  3.8440e-01,  7.7994e-01,  1.3179e+00, -4.9912e-01,\n",
            "          1.0018e+00,  4.5087e-01, -5.2782e-01, -1.1475e+00,  2.4739e-01,\n",
            "          7.4257e-01,  1.8414e+00,  1.6018e+00,  1.2646e+00, -3.2053e-01,\n",
            "          1.0670e+00,  4.6982e-01,  2.4911e-01,  6.0531e-01,  4.7326e-01,\n",
            "          1.5216e+00,  8.2656e-01, -7.2974e-01,  4.5620e-01,  1.1453e+00,\n",
            "          1.1329e+00,  1.4173e+00,  1.5771e+00, -3.4323e-01, -3.2225e-01,\n",
            "          1.1084e+00, -4.0405e-01,  3.2445e-02, -2.3621e-01,  1.3215e+00,\n",
            "          5.0946e-02,  1.2978e+00,  9.3306e-01, -1.8683e-01, -4.7418e-01,\n",
            "          4.7263e-01,  1.4788e-02, -4.9277e-01,  1.5719e+00, -2.0191e-01,\n",
            "          9.2586e-01, -1.5548e+00,  7.9181e-01, -8.9760e-01, -2.2788e+00,\n",
            "          4.7358e-01,  1.8179e+00, -3.2106e-01, -2.7238e-01,  1.4299e+00,\n",
            "          1.1133e+00, -1.0761e-01,  7.2931e-01,  1.2643e+00,  2.4946e-01,\n",
            "          3.0975e-01, -5.9270e-01,  5.6231e-02, -7.9685e-01,  5.4154e-01,\n",
            "         -3.0611e-01,  1.2011e-01,  8.3585e-01,  5.7423e-01, -6.6148e-01,\n",
            "         -7.4112e-01,  1.2797e+00,  3.8663e-01,  1.7785e+00,  1.9641e+00,\n",
            "         -8.3786e-01, -5.1261e-01,  1.5772e+00,  1.0081e+00,  9.6557e-01,\n",
            "         -3.2137e-02, -3.0724e-01,  1.2496e+00, -1.0453e+00,  7.1161e-01,\n",
            "          1.3912e+00,  8.9947e-01,  7.9251e-01, -4.2937e-01, -1.8637e+00,\n",
            "         -3.1412e-02, -1.3338e-01,  1.7893e-01,  5.9726e-01,  4.7931e-02,\n",
            "         -4.0506e-01,  8.7491e-01, -8.2017e-01,  4.3828e-01, -2.4717e-01,\n",
            "         -7.5339e-01, -8.8535e-01, -5.8322e-01, -5.9220e-02,  1.4395e+00,\n",
            "         -1.0512e-01, -9.4514e-02,  2.7955e-01, -1.7770e+00, -1.4767e-01,\n",
            "         -8.8519e-01,  2.8168e-01,  3.0322e-01, -2.6221e-01, -4.0756e-02,\n",
            "         -4.6399e-01, -1.0078e+00, -1.2145e-01,  3.1880e-01, -7.4603e-01,\n",
            "         -6.5447e-01, -1.4847e+00,  4.6332e-01,  5.6712e-01, -4.9967e-01,\n",
            "         -1.4650e-01, -6.9721e-01, -6.6883e-01, -8.6694e-02,  6.9289e-01,\n",
            "         -7.1076e-01, -4.5568e-01, -6.0302e-01,  5.7206e-02, -9.0665e-01,\n",
            "          1.7857e-01,  3.0253e-01, -4.7176e-01, -9.5180e-01, -1.5440e+00,\n",
            "         -2.3772e-01,  4.8617e-01, -7.7127e-01,  8.4122e-01,  8.5636e-02,\n",
            "         -1.5043e-01,  9.5140e-01, -4.3727e-01, -6.7261e-01, -2.2540e+00,\n",
            "          6.8577e-01, -1.5013e+00,  3.8805e-01,  6.8862e-02, -8.8200e-01,\n",
            "         -6.5444e-01, -2.4495e-01,  9.1926e-01, -6.7129e-01, -8.6625e-01,\n",
            "         -1.4445e+00, -2.6917e+00,  1.3139e+00, -1.9175e-01, -7.9719e-01,\n",
            "         -5.5457e-01, -1.3512e+00, -9.7679e-01, -2.0870e+00, -7.7603e-01,\n",
            "         -4.7766e-01, -8.3378e-02, -4.5023e-01,  1.0615e+00,  8.8473e-01]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction.dtype)"
      ],
      "metadata": {
        "id": "14xCkHE8MiKA",
        "outputId": "d86c1eff-49e2-4283-b56e-d9c564d2aff1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prediction.shape)"
      ],
      "metadata": {
        "id": "1u4LOp4ZMt4h",
        "outputId": "6a1715d4-606d-4db9-e97b-d4ed36c0f94c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2ZLvIQPIv8T"
      },
      "source": [
        "We use the model\\'s prediction and the corresponding label to calculate\n",
        "the error (`loss`). The next step is to backpropagate this error through\n",
        "the network. Backward propagation is kicked off when we call\n",
        "`.backward()` on the error tensor. Autograd then calculates and stores\n",
        "the gradients for each model parameter in the parameter\\'s `.grad`\n",
        "attribute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "powPIx9WIv8T"
      },
      "outputs": [],
      "source": [
        "loss = (prediction - labels).sum()\n",
        "loss.backward() # backward pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss)"
      ],
      "metadata": {
        "id": "0rwvC9dgL9up",
        "outputId": "2dac3e77-99bd-4b97-ae4c-86aebfecb36c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-485.6462, grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss.dtype)\n",
        "print(loss.shape)"
      ],
      "metadata": {
        "id": "zqTLSss_MzwO",
        "outputId": "052e1319-cff6-4462-ca01-5868007e7a5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n",
            "torch.Size([])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QNMK1De_NBDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ygp5Kdh4Mxlm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OECngCjzIv8U"
      },
      "source": [
        "Next, we load an optimizer, in this case SGD with a learning rate of\n",
        "0.01 and\n",
        "[momentum](https://medium.com/data-science/stochastic-gradient-descent-with-momentum-a84097641a5d)\n",
        "of 0.9. We register all the parameters of the model in the optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kgx_ch9nIv8U"
      },
      "outputs": [],
      "source": [
        "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(optim)"
      ],
      "metadata": {
        "id": "otKLQoHZNHWC",
        "outputId": "62877c2e-52b1-428f-d32c-281690970a5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SGD (\n",
            "Parameter Group 0\n",
            "    dampening: 0\n",
            "    differentiable: False\n",
            "    foreach: None\n",
            "    fused: None\n",
            "    lr: 0.01\n",
            "    maximize: False\n",
            "    momentum: 0.9\n",
            "    nesterov: False\n",
            "    weight_decay: 0\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHpvFaqDIv8U"
      },
      "source": [
        "Finally, we call `.step()` to initiate gradient descent. The optimizer\n",
        "adjusts each parameter by its gradient stored in `.grad`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "r-gqSxiPIv8U"
      },
      "outputs": [],
      "source": [
        "optim.step() #gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Ar9CVVIv8V"
      },
      "source": [
        "At this point, you have everything you need to train your neural\n",
        "network. The below sections detail the workings of autograd - feel free\n",
        "to skip them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7_GSKBVIv8V"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU2F0Y6qIv8V"
      },
      "source": [
        "Differentiation in Autograd\n",
        "===========================\n",
        "\n",
        "Let\\'s take a look at how `autograd` collects gradients. We create two\n",
        "tensors `a` and `b` with `requires_grad=True`. This signals to\n",
        "`autograd` that every operation on them should be tracked.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "PjNymfP8Iv8V"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "a = torch.tensor([2., 3.], requires_grad=True)\n",
        "b = torch.tensor([6., 4.], requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a)"
      ],
      "metadata": {
        "id": "Ce-oo-QDNSWD",
        "outputId": "950da021-668b-4dca-bbb2-799221378f47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., 3.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(b)"
      ],
      "metadata": {
        "id": "QxfLIB-jNVzi",
        "outputId": "2c1f5666-77b9-4633-aad8-2324eeeb88f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([6., 4.], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mAuhVIfIv8V"
      },
      "source": [
        "We create another tensor `Q` from `a` and `b`.\n",
        "\n",
        "$$Q = 3a^3 - b^2$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8vY8tJx9Iv8V"
      },
      "outputs": [],
      "source": [
        "Q = 3*a**3 - b**2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "metadata": {
        "id": "37sL2SeZN4m9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q"
      ],
      "metadata": {
        "id": "r-TxZBRHN_J4",
        "outputId": "15c1bbd4-7dab-4bc2-ff83-6a8562928d90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-12.,  65.], grad_fn=<SubBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPkNcCQ5Iv8V"
      },
      "source": [
        "Let\\'s assume `a` and `b` to be parameters of an NN, and `Q` to be the\n",
        "error. In NN training, we want gradients of the error w.r.t. parameters,\n",
        "i.e.\n",
        "\n",
        "$$\\frac{\\partial Q}{\\partial a} = 9a^2$$\n",
        "\n",
        "$$\\frac{\\partial Q}{\\partial b} = -2b$$\n",
        "\n",
        "When we call `.backward()` on `Q`, autograd calculates these gradients\n",
        "and stores them in the respective tensors\\' `.grad` attribute.\n",
        "\n",
        "We need to explicitly pass a `gradient` argument in `Q.backward()`\n",
        "because it is a vector. `gradient` is a tensor of the same shape as `Q`,\n",
        "and it represents the gradient of Q w.r.t. itself, i.e.\n",
        "\n",
        "$$\\frac{dQ}{dQ} = 1$$\n",
        "\n",
        "Equivalently, we can also aggregate Q into a scalar and call backward\n",
        "implicitly, like `Q.sum().backward()`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "jWvbqjwzIv8V"
      },
      "outputs": [],
      "source": [
        "external_grad = torch.tensor([1., 1.])\n",
        "Q.backward(gradient=external_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXU8pq7xIv8V"
      },
      "source": [
        "Gradients are now deposited in `a.grad` and `b.grad`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "xsKgMal1Iv8W",
        "outputId": "068e78bd-0d45-402a-e9c5-f6beee9ce9aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([True, True])\n",
            "tensor([True, True])\n"
          ]
        }
      ],
      "source": [
        "# check if collected gradients are correct\n",
        "print(9*a**2 == a.grad)\n",
        "print(-2*b == b.grad)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.grad)"
      ],
      "metadata": {
        "id": "PwsddWcYPgKv",
        "outputId": "c8612d3c-83fc-42ea-b3d4-42f8c5b91d34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([36., 81.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "\n",
        "\n",
        "def _shape(x):\n",
        "    \"\"\"把 forward 输出转成易读的 shape 字符串（兼容 Tensor / tuple / list / dict / None）\"\"\"\n",
        "    if torch.is_tensor(x):\n",
        "        return tuple(x.shape)\n",
        "    if isinstance(x, (list, tuple)):\n",
        "        # 只展示第一个元素的 shape，避免输出太长\n",
        "        if len(x) == 0:\n",
        "            return \"[]\"\n",
        "        return f\"{type(x).__name__}[0]:\" + str(_shape(x[0]))\n",
        "    if isinstance(x, dict):\n",
        "        if len(x) == 0:\n",
        "            return \"{}\"\n",
        "        k = next(iter(x.keys()))\n",
        "        return f\"dict['{k}']:\" + str(_shape(x[k]))\n",
        "    return str(type(x).__name__)\n",
        "\n",
        "\n",
        "def _describe_module(m: torch.nn.Module) -> str:\n",
        "    \"\"\"把常见层的关键超参数提取出来，做成一行摘要\"\"\"\n",
        "    import torch.nn as nn\n",
        "\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        return (\n",
        "            f\"Conv2d({m.in_channels}→{m.out_channels}, \"\n",
        "            f\"k={tuple(m.kernel_size)}, s={tuple(m.stride)}, p={tuple(m.padding)}, \"\n",
        "            f\"bias={m.bias is not None})\"\n",
        "        )\n",
        "    if isinstance(m, nn.BatchNorm2d):\n",
        "        return (\n",
        "            f\"BatchNorm2d({m.num_features}, eps={m.eps}, momentum={m.momentum}, \"\n",
        "            f\"affine={m.affine}, track_running_stats={m.track_running_stats})\"\n",
        "        )\n",
        "    if isinstance(m, nn.ReLU):\n",
        "        return f\"ReLU(inplace={m.inplace})\"\n",
        "    if isinstance(m, nn.MaxPool2d):\n",
        "        return (\n",
        "            f\"MaxPool2d(k={m.kernel_size}, s={m.stride}, p={m.padding}, \"\n",
        "            f\"dilation={m.dilation}, ceil_mode={m.ceil_mode})\"\n",
        "        )\n",
        "    if isinstance(m, nn.AdaptiveAvgPool2d):\n",
        "        return f\"AdaptiveAvgPool2d(output_size={m.output_size})\"\n",
        "    if isinstance(m, nn.Linear):\n",
        "        return f\"Linear({m.in_features}→{m.out_features}, bias={m.bias is not None})\"\n",
        "    if isinstance(m, nn.Sequential):\n",
        "        return f\"Sequential(n={len(m)})\"\n",
        "\n",
        "    # ResNet18 的残差块类型（torchvision.models.resnet.BasicBlock）\n",
        "    if m.__class__.__name__ == \"BasicBlock\":\n",
        "        # BasicBlock 常用字段：stride / downsample（有些版本还有 dilation 等）\n",
        "        stride = getattr(m, \"stride\", None)\n",
        "        downsample = getattr(m, \"downsample\", None)\n",
        "        has_downsample = downsample is not None\n",
        "        return f\"BasicBlock(stride={stride}, downsample={has_downsample})\"\n",
        "\n",
        "    return m.__class__.__name__\n",
        "\n",
        "\n",
        "def _annotation(name: str) -> str:\n",
        "    \"\"\"给常见层名加中文注释（你也可以按需扩充）\"\"\"\n",
        "    ann = {\n",
        "        \"conv1\": \"Stem：首层 7×7 卷积（降采样、扩通道）\",\n",
        "        \"bn1\": \"Stem：BatchNorm（稳定训练/推理）\",\n",
        "        \"relu\": \"Stem：ReLU 非线性\",\n",
        "        \"maxpool\": \"Stem：最大池化（再次降采样）\",\n",
        "        \"layer1\": \"Stage 1：残差块堆叠（不降采样，通道 64）\",\n",
        "        \"layer2\": \"Stage 2：残差块堆叠（首块降采样，通道 128）\",\n",
        "        \"layer3\": \"Stage 3：残差块堆叠（首块降采样，通道 256）\",\n",
        "        \"layer4\": \"Stage 4：残差块堆叠（首块降采样，通道 512）\",\n",
        "        \"avgpool\": \"Head：自适应全局平均池化到 1×1\",\n",
        "        \"fc\": \"Head：全连接分类层（ImageNet 默认 1000 类）\",\n",
        "        \"downsample\": \"残差分支：用 1×1 conv/BN 对齐形状以便相加\",\n",
        "        \"conv2\": \"残差主分支：第二个 3×3 卷积\",\n",
        "        \"bn2\": \"残差主分支：第二个 BatchNorm\",\n",
        "    }\n",
        "    return ann.get(name, \"\")\n",
        "\n",
        "\n",
        "def register_shape_hooks(model: torch.nn.Module):\n",
        "    \"\"\"\n",
        "    给关键模块注册 forward hook，用于记录每层输出 shape：\n",
        "    - 记录：stem(各层) / layer1-4(整体) / 每个 BasicBlock / head\n",
        "    \"\"\"\n",
        "    shapes = {}\n",
        "\n",
        "    def _hook(name):\n",
        "        def fn(_m, _inp, out):\n",
        "            shapes[name] = _shape(out)\n",
        "        return fn\n",
        "\n",
        "    # 你关心的“结构层级节点”\n",
        "    target_names = [\n",
        "        \"conv1\", \"bn1\", \"relu\", \"maxpool\",\n",
        "        \"layer1\", \"layer2\", \"layer3\", \"layer4\",\n",
        "        \"avgpool\", \"fc\",\n",
        "        # 每个 stage 的两个 block（ResNet-18 固定 2 个）\n",
        "        \"layer1.0\", \"layer1.1\",\n",
        "        \"layer2.0\", \"layer2.1\",\n",
        "        \"layer3.0\", \"layer3.1\",\n",
        "        \"layer4.0\", \"layer4.1\",\n",
        "    ]\n",
        "\n",
        "    # 对存在的子模块挂 hook\n",
        "    for n in target_names:\n",
        "        m = model.get_submodule(n)\n",
        "        m.register_forward_hook(_hook(n))\n",
        "\n",
        "    return shapes\n",
        "\n",
        "\n",
        "def print_tree(model: torch.nn.Module, shapes: dict, input_shape):\n",
        "    \"\"\"\n",
        "    结构化打印：\n",
        "    - 顶层信息\n",
        "    - Stem\n",
        "    - 4 个 stage（含 block 级别信息）\n",
        "    - Head\n",
        "    \"\"\"\n",
        "    print(\"=\" * 88)\n",
        "    print(\"Model:\", model.__class__.__name__, \"(torchvision ResNet-18)\")\n",
        "    print(\"Input:\", input_shape, \"  # 例如 (1, 3, 64, 64)\")\n",
        "    print(\"=\" * 88)\n",
        "\n",
        "    def line(path, module, indent=0):\n",
        "        name = path.split(\".\")[-1]\n",
        "        desc = _describe_module(module)\n",
        "        ann = _annotation(name)\n",
        "        shp = shapes.get(path, None)\n",
        "        shp_s = f\" | out={shp}\" if shp is not None else \"\"\n",
        "        ann_s = f\"  # {ann}\" if ann else \"\"\n",
        "        print(\"  \" * indent + f\"- {path}: {desc}{shp_s}{ann_s}\")\n",
        "\n",
        "    # --- Stem ---\n",
        "    print(\"\\n[Stem]  # 特征提取入口（快速降采样 + 提升通道）\")\n",
        "    for n in [\"conv1\", \"bn1\", \"relu\", \"maxpool\"]:\n",
        "        line(n, model.get_submodule(n), indent=0)\n",
        "\n",
        "    # --- Stages ---\n",
        "    for stage in [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]:\n",
        "        print(f\"\\n[{stage}]  # { _annotation(stage) }\")\n",
        "        line(stage, model.get_submodule(stage), indent=0)\n",
        "\n",
        "        # 每个 stage 的 block（ResNet-18：2 个）\n",
        "        for i in [0, 1]:\n",
        "            bname = f\"{stage}.{i}\"\n",
        "            block = model.get_submodule(bname)\n",
        "            line(bname, block, indent=1)\n",
        "\n",
        "            # block 内部结构（固定字段：conv1/bn1/relu/conv2/bn2 + 可选 downsample）\n",
        "            for sub in [\"conv1\", \"bn1\", \"relu\", \"conv2\", \"bn2\"]:\n",
        "                if hasattr(block, sub):\n",
        "                    line(f\"{bname}.{sub}\", getattr(block, sub), indent=2)\n",
        "            if getattr(block, \"downsample\", None) is not None:\n",
        "                # downsample 是 Sequential，里面一般是 1×1 conv + BN\n",
        "                ds = block.downsample\n",
        "                line(f\"{bname}.downsample\", ds, indent=2)\n",
        "                for j, child in enumerate(ds):\n",
        "                    line(f\"{bname}.downsample.{j}\", child, indent=3)\n",
        "\n",
        "    # --- Head ---\n",
        "    print(\"\\n[Head]  # 池化 + 分类器\")\n",
        "    for n in [\"avgpool\", \"fc\"]:\n",
        "        line(n, model.get_submodule(n), indent=0)\n",
        "\n",
        "    print(\"\\n备注：out=... 是根据一次前向传播自动记录的输出 shape（和输入大小有关）。\")\n",
        "    print(\"=\" * 88)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # 1) 构建模型（ImageNet 预训练权重）\n",
        "    model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "\n",
        "    # 2) 构造你的输入（示例：1×3×64×64）\n",
        "    data = torch.rand(1, 3, 64, 64)\n",
        "\n",
        "    # 3) 注册 hook 以便自动记录每层输出 shape\n",
        "    shapes = register_shape_hooks(model)\n",
        "\n",
        "    # 4) 做一次前向传播（eval + no_grad：避免 BN 统计更新 & 不建梯度图）\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        _ = model(data)\n",
        "\n",
        "    # 5) 结构化打印（带中文注释 + 输出 shape）\n",
        "    print_tree(model, shapes, input_shape=tuple(data.shape))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "5CSDBNdqQ18J",
        "outputId": "ce68eb42-327b-4b04-8a25-1dfd2bca0d8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================================================================\n",
            "Model: ResNet (torchvision ResNet-18)\n",
            "Input: (1, 3, 64, 64)   # 例如 (1, 3, 64, 64)\n",
            "========================================================================================\n",
            "\n",
            "[Stem]  # 特征提取入口（快速降采样 + 提升通道）\n",
            "- conv1: Conv2d(3→64, k=(7, 7), s=(2, 2), p=(3, 3), bias=False) | out=(1, 64, 32, 32)  # Stem：首层 7×7 卷积（降采样、扩通道）\n",
            "- bn1: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) | out=(1, 64, 32, 32)  # Stem：BatchNorm（稳定训练/推理）\n",
            "- relu: ReLU(inplace=True) | out=(1, 64, 32, 32)  # Stem：ReLU 非线性\n",
            "- maxpool: MaxPool2d(k=3, s=2, p=1, dilation=1, ceil_mode=False) | out=(1, 64, 16, 16)  # Stem：最大池化（再次降采样）\n",
            "\n",
            "[layer1]  # Stage 1：残差块堆叠（不降采样，通道 64）\n",
            "- layer1: Sequential(n=2) | out=(1, 64, 16, 16)  # Stage 1：残差块堆叠（不降采样，通道 64）\n",
            "  - layer1.0: BasicBlock(stride=1, downsample=False) | out=(1, 64, 16, 16)\n",
            "    - layer1.0.conv1: Conv2d(64→64, k=(3, 3), s=(1, 1), p=(1, 1), bias=False)  # Stem：首层 7×7 卷积（降采样、扩通道）\n",
            "    - layer1.0.bn1: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # Stem：BatchNorm（稳定训练/推理）\n",
            "    - layer1.0.relu: ReLU(inplace=True)  # Stem：ReLU 非线性\n",
            "    - layer1.0.conv2: Conv2d(64→64, k=(3, 3), s=(1, 1), p=(1, 1), bias=False)  # 残差主分支：第二个 3×3 卷积\n",
            "    - layer1.0.bn2: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # 残差主分支：第二个 BatchNorm\n",
            "  - layer1.1: BasicBlock(stride=1, downsample=False) | out=(1, 64, 16, 16)\n",
            "    - layer1.1.conv1: Conv2d(64→64, k=(3, 3), s=(1, 1), p=(1, 1), bias=False)  # Stem：首层 7×7 卷积（降采样、扩通道）\n",
            "    - layer1.1.bn1: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # Stem：BatchNorm（稳定训练/推理）\n",
            "    - layer1.1.relu: ReLU(inplace=True)  # Stem：ReLU 非线性\n",
            "    - layer1.1.conv2: Conv2d(64→64, k=(3, 3), s=(1, 1), p=(1, 1), bias=False)  # 残差主分支：第二个 3×3 卷积\n",
            "    - layer1.1.bn2: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # 残差主分支：第二个 BatchNorm\n",
            "\n",
            "[layer2]  # Stage 2：残差块堆叠（首块降采样，通道 128）\n",
            "- layer2: Sequential(n=2) | out=(1, 128, 8, 8)  # Stage 2：残差块堆叠（首块降采样，通道 128）\n",
            "  - layer2.0: BasicBlock(stride=2, downsample=True) | out=(1, 128, 8, 8)\n",
            "    - layer2.0.conv1: Conv2d(64→128, k=(3, 3), s=(2, 2), p=(1, 1), bias=False)  # Stem：首层 7×7 卷积（降采样、扩通道）\n",
            "    - layer2.0.bn1: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # Stem：BatchNorm（稳定训练/推理）\n",
            "    - layer2.0.relu: ReLU(inplace=True)  # Stem：ReLU 非线性\n",
            "    - layer2.0.conv2: Conv2d(128→128, k=(3, 3), s=(1, 1), p=(1, 1), bias=False)  # 残差主分支：第二个 3×3 卷积\n",
            "    - layer2.0.bn2: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # 残差主分支：第二个 BatchNorm\n",
            "    - layer2.0.downsample: Sequential(n=2)  # 残差分支：用 1×1 conv/BN 对齐形状以便相加\n",
            "      - layer2.0.downsample.0: Conv2d(64→128, k=(1, 1), s=(2, 2), p=(0, 0), bias=False)\n",
            "      - layer2.0.downsample.1: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  - layer2.1: BasicBlock(stride=1, downsample=False) | out=(1, 128, 8, 8)\n",
            "    - layer2.1.conv1: Conv2d(128→128, k=(3, 3), s=(1, 1), p=(1, 1), bias=False)  # Stem：首层 7×7 卷积（降采样、扩通道）\n",
            "    - layer2.1.bn1: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # Stem：BatchNorm（稳定训练/推理）\n",
            "    - layer2.1.relu: ReLU(inplace=True)  # Stem：ReLU 非线性\n",
            "    - layer2.1.conv2: Conv2d(128→128, k=(3, 3), s=(1, 1), p=(1, 1), bias=False)  # 残差主分支：第二个 3×3 卷积\n",
            "    - layer2.1.bn2: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # 残差主分支：第二个 BatchNorm\n",
            "\n",
            "[layer3]  # Stage 3：残差块堆叠（首块降采样，通道 256）\n",
            "- layer3: Sequential(n=2) | out=(1, 256, 4, 4)  # Stage 3：残差块堆叠（首块降采样，通道 256）\n",
            "  - layer3.0: BasicBlock(stride=2, downsample=True) | out=(1, 256, 4, 4)\n",
            "    - layer3.0.conv1: Conv2d(128→256, k=(3, 3), s=(2, 2), p=(1, 1), bias=False)  # Stem：首层 7×7 卷积（降采样、扩通道）\n",
            "    - layer3.0.bn1: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # Stem：BatchNorm（稳定训练/推理）\n",
            "    - layer3.0.relu: ReLU(inplace=True)  # Stem：ReLU 非线性\n",
            "    - layer3.0.conv2: Conv2d(256→256, k=(3, 3), s=(1, 1), p=(1, 1), bias=False)  # 残差主分支：第二个 3×3 卷积\n",
            "    - layer3.0.bn2: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # 残差主分支：第二个 BatchNorm\n",
            "    - layer3.0.downsample: Sequential(n=2)  # 残差分支：用 1×1 conv/BN 对齐形状以便相加\n",
            "      - layer3.0.downsample.0: Conv2d(128→256, k=(1, 1), s=(2, 2), p=(0, 0), bias=False)\n",
            "      - layer3.0.downsample.1: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  - layer3.1: BasicBlock(stride=1, downsample=False) | out=(1, 256, 4, 4)\n",
            "    - layer3.1.conv1: Conv2d(256→256, k=(3, 3), s=(1, 1), p=(1, 1), bias=False)  # Stem：首层 7×7 卷积（降采样、扩通道）\n",
            "    - layer3.1.bn1: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # Stem：BatchNorm（稳定训练/推理）\n",
            "    - layer3.1.relu: ReLU(inplace=True)  # Stem：ReLU 非线性\n",
            "    - layer3.1.conv2: Conv2d(256→256, k=(3, 3), s=(1, 1), p=(1, 1), bias=False)  # 残差主分支：第二个 3×3 卷积\n",
            "    - layer3.1.bn2: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # 残差主分支：第二个 BatchNorm\n",
            "\n",
            "[layer4]  # Stage 4：残差块堆叠（首块降采样，通道 512）\n",
            "- layer4: Sequential(n=2) | out=(1, 512, 2, 2)  # Stage 4：残差块堆叠（首块降采样，通道 512）\n",
            "  - layer4.0: BasicBlock(stride=2, downsample=True) | out=(1, 512, 2, 2)\n",
            "    - layer4.0.conv1: Conv2d(256→512, k=(3, 3), s=(2, 2), p=(1, 1), bias=False)  # Stem：首层 7×7 卷积（降采样、扩通道）\n",
            "    - layer4.0.bn1: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # Stem：BatchNorm（稳定训练/推理）\n",
            "    - layer4.0.relu: ReLU(inplace=True)  # Stem：ReLU 非线性\n",
            "    - layer4.0.conv2: Conv2d(512→512, k=(3, 3), s=(1, 1), p=(1, 1), bias=False)  # 残差主分支：第二个 3×3 卷积\n",
            "    - layer4.0.bn2: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # 残差主分支：第二个 BatchNorm\n",
            "    - layer4.0.downsample: Sequential(n=2)  # 残差分支：用 1×1 conv/BN 对齐形状以便相加\n",
            "      - layer4.0.downsample.0: Conv2d(256→512, k=(1, 1), s=(2, 2), p=(0, 0), bias=False)\n",
            "      - layer4.0.downsample.1: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  - layer4.1: BasicBlock(stride=1, downsample=False) | out=(1, 512, 2, 2)\n",
            "    - layer4.1.conv1: Conv2d(512→512, k=(3, 3), s=(1, 1), p=(1, 1), bias=False)  # Stem：首层 7×7 卷积（降采样、扩通道）\n",
            "    - layer4.1.bn1: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # Stem：BatchNorm（稳定训练/推理）\n",
            "    - layer4.1.relu: ReLU(inplace=True)  # Stem：ReLU 非线性\n",
            "    - layer4.1.conv2: Conv2d(512→512, k=(3, 3), s=(1, 1), p=(1, 1), bias=False)  # 残差主分支：第二个 3×3 卷积\n",
            "    - layer4.1.bn2: BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)  # 残差主分支：第二个 BatchNorm\n",
            "\n",
            "[Head]  # 池化 + 分类器\n",
            "- avgpool: AdaptiveAvgPool2d(output_size=(1, 1)) | out=(1, 512, 1, 1)  # Head：自适应全局平均池化到 1×1\n",
            "- fc: Linear(512→1000, bias=True) | out=(1, 1000)  # Head：全连接分类层（ImageNet 默认 1000 类）\n",
            "\n",
            "备注：out=... 是根据一次前向传播自动记录的输出 shape（和输入大小有关）。\n",
            "========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Drkl5aPIv8W"
      },
      "source": [
        "Optional Reading - Vector Calculus using `autograd`\n",
        "===================================================\n",
        "\n",
        "Mathematically, if you have a vector valued function\n",
        "$\\vec{y}=f(\\vec{x})$, then the gradient of $\\vec{y}$ with respect to\n",
        "$\\vec{x}$ is a Jacobian matrix $J$:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "J\n",
        "=\n",
        " \\left(\\begin{array}{cc}\n",
        " \\frac{\\partial \\bf{y}}{\\partial x_{1}} &\n",
        " ... &\n",
        " \\frac{\\partial \\bf{y}}{\\partial x_{n}}\n",
        " \\end{array}\\right)\n",
        "=\n",
        "\\left(\\begin{array}{ccc}\n",
        " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
        " \\vdots & \\ddots & \\vdots\\\\\n",
        " \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        " \\end{array}\\right)\n",
        "\\end{aligned}$$\n",
        "\n",
        "Generally speaking, `torch.autograd` is an engine for computing\n",
        "vector-Jacobian product. That is, given any vector $\\vec{v}$, compute\n",
        "the product $J^{T}\\cdot \\vec{v}$\n",
        "\n",
        "If $\\vec{v}$ happens to be the gradient of a scalar function\n",
        "$l=g\\left(\\vec{y}\\right)$:\n",
        "\n",
        "$$\\vec{v}\n",
        " =\n",
        " \\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}$$\n",
        "\n",
        "then by the chain rule, the vector-Jacobian product would be the\n",
        "gradient of $l$ with respect to $\\vec{x}$:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "J^{T}\\cdot \\vec{v} = \\left(\\begin{array}{ccc}\n",
        " \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
        " \\vdots & \\ddots & \\vdots\\\\\n",
        " \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
        " \\end{array}\\right)\\left(\\begin{array}{c}\n",
        " \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
        " \\vdots\\\\\n",
        " \\frac{\\partial l}{\\partial y_{m}}\n",
        " \\end{array}\\right) = \\left(\\begin{array}{c}\n",
        " \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
        " \\vdots\\\\\n",
        " \\frac{\\partial l}{\\partial x_{n}}\n",
        " \\end{array}\\right)\n",
        "\\end{aligned}$$\n",
        "\n",
        "This characteristic of vector-Jacobian product is what we use in the\n",
        "above example; `external_grad` represents $\\vec{v}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mijOqKJ_Iv8W"
      },
      "source": [
        "Computational Graph\n",
        "===================\n",
        "\n",
        "Conceptually, autograd keeps a record of data (tensors) & all executed\n",
        "operations (along with the resulting new tensors) in a directed acyclic\n",
        "graph (DAG) consisting of\n",
        "[Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)\n",
        "objects. In this DAG, leaves are the input tensors, roots are the output\n",
        "tensors. By tracing this graph from roots to leaves, you can\n",
        "automatically compute the gradients using the chain rule.\n",
        "\n",
        "In a forward pass, autograd does two things simultaneously:\n",
        "\n",
        "-   run the requested operation to compute a resulting tensor, and\n",
        "-   maintain the operation's *gradient function* in the DAG.\n",
        "\n",
        "The backward pass kicks off when `.backward()` is called on the DAG\n",
        "root. `autograd` then:\n",
        "\n",
        "-   computes the gradients from each `.grad_fn`,\n",
        "-   accumulates them in the respective tensor's `.grad` attribute, and\n",
        "-   using the chain rule, propagates all the way to the leaf tensors.\n",
        "\n",
        "Below is a visual representation of the DAG in our example. In the\n",
        "graph, the arrows are in the direction of the forward pass. The nodes\n",
        "represent the backward functions of each operation in the forward pass.\n",
        "The leaf nodes in blue represent our leaf tensors `a` and `b`.\n",
        "\n",
        "![](https://pytorch.org/tutorials/_static/img/dag_autograd.png)\n",
        "\n",
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>An important thing to note is that the graph is recreated from scratch; after each<code>.backward()</code> call, autograd starts populating a new graph. This isexactly what allows you to use control flow statements in your model;you can change the shape, size and operations at every iteration ifneeded.</p>\n",
        "\n",
        "</div>\n",
        "\n",
        "Exclusion from the DAG\n",
        "----------------------\n",
        "\n",
        "`torch.autograd` tracks operations on all tensors which have their\n",
        "`requires_grad` flag set to `True`. For tensors that don't require\n",
        "gradients, setting this attribute to `False` excludes it from the\n",
        "gradient computation DAG.\n",
        "\n",
        "The output tensor of an operation will require gradients even if only a\n",
        "single input tensor has `requires_grad=True`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6KBeD2GPIv8W",
        "outputId": "0b91e26e-d10c-4fdf-d9b9-e4fbcd2d9fff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Does `a` require gradients?: False\n",
            "Does `b` require gradients?: True\n"
          ]
        }
      ],
      "source": [
        "x = torch.rand(5, 5)\n",
        "y = torch.rand(5, 5)\n",
        "z = torch.rand((5, 5), requires_grad=True)\n",
        "\n",
        "a = x + y\n",
        "print(f\"Does `a` require gradients?: {a.requires_grad}\")\n",
        "b = x + z\n",
        "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y41glOv_Iv8W"
      },
      "source": [
        "In a NN, parameters that don\\'t compute gradients are usually called\n",
        "**frozen parameters**. It is useful to \\\"freeze\\\" part of your model if\n",
        "you know in advance that you won\\'t need the gradients of those\n",
        "parameters (this offers some performance benefits by reducing autograd\n",
        "computations).\n",
        "\n",
        "In finetuning, we freeze most of the model and typically only modify the\n",
        "classifier layers to make predictions on new labels. Let\\'s walk through\n",
        "a small example to demonstrate this. As before, we load a pretrained\n",
        "resnet18 model, and freeze all the parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-8XgXz0zIv8W"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "\n",
        "# Freeze all the parameters in the network\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czcy4cSnIv8W"
      },
      "source": [
        "Let\\'s say we want to finetune the model on a new dataset with 10\n",
        "labels. In resnet, the classifier is the last linear layer `model.fc`.\n",
        "We can simply replace it with a new linear layer (unfrozen by default)\n",
        "that acts as our classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "E1HKsGXvIv8W"
      },
      "outputs": [],
      "source": [
        "model.fc = nn.Linear(512, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp25LsTHIv8W"
      },
      "source": [
        "Now all parameters in the model, except the parameters of `model.fc`,\n",
        "are frozen. The only parameters that compute gradients are the weights\n",
        "and bias of `model.fc`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "OMUOWPw0Iv8X"
      },
      "outputs": [],
      "source": [
        "# Optimize only the classifier\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHy1JuSyIv8X"
      },
      "source": [
        "Notice although we register all the parameters in the optimizer, the\n",
        "only parameters that are computing gradients (and hence updated in\n",
        "gradient descent) are the weights and bias of the classifier.\n",
        "\n",
        "The same exclusionary functionality is available as a context manager in\n",
        "[torch.no\\_grad()](https://pytorch.org/docs/stable/generated/torch.no_grad.html)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stzz6qs6Iv8X"
      },
      "source": [
        "------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZGQK6mfIv8X"
      },
      "source": [
        "Further readings:\n",
        "=================\n",
        "\n",
        "-   [In-place operations & Multithreaded\n",
        "    Autograd](https://pytorch.org/docs/stable/notes/autograd.html)\n",
        "-   [Example implementation of reverse-mode\n",
        "    autodiff](https://colab.research.google.com/drive/1VpeE6UvEPRz9HmsHh1KS0XxXjYu533EC)\n",
        "-   [Video: PyTorch Autograd Explained - In-depth\n",
        "    Tutorial](https://www.youtube.com/watch?v=MswxJw-8PvE)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}