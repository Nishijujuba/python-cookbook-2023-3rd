{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishijujuba/python-cookbook-2023-3rd/blob/master/_downloads/c029676472d90691aa145c6fb97a61c3/neural_networks_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TipQOtCueWKI"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://docs.pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pxAr4hBeWKQ"
      },
      "source": [
        "Neural Networks\n",
        "===============\n",
        "\n",
        "Neural networks can be constructed using the `torch.nn` package.\n",
        "\n",
        "Now that you had a glimpse of `autograd`, `nn` depends on `autograd` to\n",
        "define models and differentiate them. An `nn.Module` contains layers,\n",
        "and a method `forward(input)` that returns the `output`.\n",
        "\n",
        "For example, look at this network that classifies digit images:\n",
        "\n",
        "![convnet](https://pytorch.org/tutorials/_static/img/mnist.png)\n",
        "\n",
        "It is a simple feed-forward network. It takes the input, feeds it\n",
        "through several layers one after the other, and then finally gives the\n",
        "output.\n",
        "\n",
        "A typical training procedure for a neural network is as follows:\n",
        "\n",
        "-   Define the neural network that has some learnable parameters (or\n",
        "    weights)\n",
        "-   Iterate over a dataset of inputs\n",
        "-   Process input through the network\n",
        "-   Compute the loss (how far is the output from being correct)\n",
        "-   Propagate gradients back into the network's parameters\n",
        "-   Update the weights of the network, typically using a simple update\n",
        "    rule: `weight = weight - learning_rate * gradient`\n",
        "\n",
        "Define the network\n",
        "------------------\n",
        "\n",
        "Let's define this network:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kPwxNsyXeWKU",
        "outputId": "dbdb7a9c-b8f8-45a2-acf4-55765c4de253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):#定义一个网络类，继承nn.Module，这样Pytorch才可以：注册参数、支持相关方法.to(device)、 .state_dict()、.parameters()等\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 调用父类初始化：让 nn.Module 做好内部注册/管理的准备\n",
        "\n",
        "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        # 卷积层1：输入通道=1（灰度图），输出通道=6，卷积核大小=5\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # 卷积层2：输入通道=6，输出通道=16，卷积核=5\n",
        "\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension\n",
        "        # 全连接层1：输入特征=16*5*5=400，输出=120，这个 5*5 来自前面卷积+池化后的空间尺寸推导（见 forward）\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        # 全连接层2：120 -> 84\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        # 输出层：84 -> 10（10 类数字 0~9）\n",
        "\n",
        "    def forward(self, input):\n",
        "        # 定义前向传播：描述“从 input 到 output 怎么算”\n",
        "\n",
        "\n",
        "        # Convolution layer C1: 1 input image channel, 6 output channels,\n",
        "        # 5x5 square convolution, it uses RELU activation function, and\n",
        "        # outputs a Tensor with size (N, 6, 28, 28), where N is the size of the batch\n",
        "        c1 = F.relu(self.conv1(input))\n",
        "        # self.conv1: (N,1,32,32) -> (N,6,28,28)\n",
        "        # 解释：32 - 5 + 1 = 28（padding=0, stride=1）\n",
        "        # F.relu：逐元素 ReLU，不改变形状\n",
        "\n",
        "        # Subsampling layer S2: 2x2 grid, purely functional,\n",
        "        # this layer does not have any parameter, and outputs a (N, 6, 14, 14) Tensor\n",
        "        s2 = F.max_pool2d(c1, (2, 2))\n",
        "        # 2x2 最大池化： (N,6,28,28) -> (N,6,14,14)\n",
        "        # 解释：默认 stride=kernel_size=2，空间尺寸减半\n",
        "        # 池化层本身没有可学习参数\n",
        "\n",
        "        # Convolution layer C3: 6 input channels, 16 output channels,\n",
        "        # 5x5 square convolution, it uses RELU activation function, and\n",
        "        # outputs a (N, 16, 10, 10) Tensor\n",
        "        c3 = F.relu(self.conv2(s2))\n",
        "        # self.conv2: (N,6,14,14) -> (N,16,10,10)\n",
        "        # 解释：14 - 5 + 1 = 10\n",
        "        # ReLU 不改形状\n",
        "\n",
        "\n",
        "        # Subsampling layer S4: 2x2 grid, purely functional,\n",
        "        # this layer does not have any parameter, and outputs a (N, 16, 5, 5) Tensor\n",
        "        s4 = F.max_pool2d(c3, 2)\n",
        "        # 池化核=2： (N,16,10,10) -> (N,16,5,5)\n",
        "\n",
        "\n",
        "        # Flatten operation: purely functional, outputs a (N, 400) Tensor\n",
        "        s4 = torch.flatten(s4, 1)\n",
        "        # 展平：从第 1 维开始展平（保留 batch 维 N）\n",
        "        # (N,16,5,5) -> (N, 16*5*5) = (N,400)\n",
        "\n",
        "\n",
        "        # Fully connected layer F5: (N, 400) Tensor input,\n",
        "        # and outputs a (N, 120) Tensor, it uses RELU activation function\n",
        "        f5 = F.relu(self.fc1(s4))\n",
        "        # fc1: (N,400) -> (N,120)，再 ReLU\n",
        "\n",
        "        # Fully connected layer F6: (N, 120) Tensor input,\n",
        "        # and outputs a (N, 84) Tensor, it uses RELU activation function\n",
        "        f6 = F.relu(self.fc2(f5))\n",
        "        # fc2: (N,120) -> (N,84)，再 ReLU\n",
        "\n",
        "        # Fully connected layer OUTPUT: (N, 84) Tensor input, and\n",
        "        # outputs a (N, 10) Tensor\n",
        "        output = self.fc3(f6)\n",
        "        # fc3: (N,84) -> (N,10)\n",
        "        # 这里通常称为 logits（未做 softmax）\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "conv1 的配置（kernel 5x5，步幅 1）。\n",
        "\n",
        "Linear(in_features=400, out_features=120, bias=True)\n",
        "fc1 输入 400 输出 120，带 bias。\n"
      ],
      "metadata": {
        "id": "z3nFnaboYwOS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-_BUIApeWKV"
      },
      "source": [
        "You just have to define the `forward` function, and the `backward`\n",
        "function (where gradients are computed) is automatically defined for you\n",
        "using `autograd`. You can use any of the Tensor operations in the\n",
        "`forward` function.\n",
        "\n",
        "The learnable parameters of a model are returned by `net.parameters()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ESWUpmKJeWKW",
        "outputId": "7420caa7-623c-497d-98a5-161887b81b66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "torch.Size([6, 1, 5, 5])\n"
          ]
        }
      ],
      "source": [
        "params = list(net.parameters())\n",
        "# net.parameters() 返回一个迭代器：依次给出所有可学习参数（weight、bias）\n",
        "# list(...) 只是为了能 len()、索引查看\n",
        "print(len(params))\n",
        "# 打印参数张量的数量（此例是 10：每个 Conv/Linear 通常都有 weight + bias）\n",
        "#5 个层（conv1、conv2、fc1、fc2、fc3）×（weight+bias）= 10 个参数张量。\n",
        "\n",
        "print(params[0].size())  # conv1's .weight\n",
        "#conv1.weight：6 个输出通道、1 个输入通道、5x5 卷积核。\n",
        "# params[0] 对应 conv1.weight，形状是 [out_channels, in_channels, kH, kW]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(params)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "whcX7DaOc_2H",
        "outputId": "656b60c4-f7e3-4da6-d3e5-2c549cb96d3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[[[ 1.8127e-01, -1.3200e-01,  1.9884e-01, -7.7197e-02,  4.1498e-02],\n",
            "          [-5.2222e-02, -1.2963e-01, -1.1134e-01, -1.0487e-01, -1.0360e-01],\n",
            "          [-1.7946e-01, -1.8686e-01, -1.9981e-01, -1.2209e-01,  1.4376e-01],\n",
            "          [-1.0527e-01, -1.5708e-01,  3.4261e-02, -3.5365e-02, -1.5402e-01],\n",
            "          [-9.5600e-02,  1.1518e-01,  3.4566e-02, -4.9534e-02,  6.7677e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 3.9205e-02,  1.2131e-01, -1.7216e-01, -1.3520e-01, -4.9865e-02],\n",
            "          [ 8.7491e-02, -1.3624e-01,  1.3771e-01, -1.3387e-01,  1.6757e-01],\n",
            "          [ 1.7481e-01, -1.3455e-01,  1.4780e-01,  1.1507e-01, -7.1423e-02],\n",
            "          [-5.3798e-02, -1.5582e-01,  9.0688e-02, -9.6122e-02,  1.0218e-01],\n",
            "          [-1.6819e-01, -1.6567e-01, -1.3055e-01, -1.7472e-01,  1.7996e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 1.7901e-01,  2.1007e-02,  1.6716e-01, -4.2629e-02,  1.3756e-01],\n",
            "          [-4.0931e-02, -7.3204e-02, -1.7830e-01,  9.7799e-02, -1.4004e-01],\n",
            "          [ 9.2205e-02, -8.2522e-02, -1.8889e-01, -1.3895e-01,  1.4628e-02],\n",
            "          [ 1.1008e-02,  1.6515e-01,  1.8111e-01, -2.4777e-02, -8.5753e-02],\n",
            "          [ 8.9079e-02, -4.6089e-02,  1.3277e-01, -1.4772e-01,  1.5288e-01]]],\n",
            "\n",
            "\n",
            "        [[[ 9.9556e-02,  8.4271e-02,  8.0674e-02, -4.4167e-02,  3.4216e-02],\n",
            "          [-6.6387e-02,  7.6650e-03, -1.9417e-01, -1.4319e-02,  7.8217e-02],\n",
            "          [ 1.1798e-01,  4.4901e-02, -5.4470e-03, -1.8672e-01,  1.4217e-01],\n",
            "          [ 1.3281e-01, -1.3910e-01, -1.3504e-02, -5.0570e-03,  1.1715e-01],\n",
            "          [-1.7806e-01,  1.0080e-02, -8.6213e-02,  1.8423e-01,  1.4174e-01]]],\n",
            "\n",
            "\n",
            "        [[[-1.2369e-01,  3.5028e-02, -1.3075e-01, -1.8801e-02,  1.2335e-01],\n",
            "          [ 2.2501e-02, -1.9719e-02,  1.3046e-01,  2.3491e-02,  1.9633e-01],\n",
            "          [-9.5133e-02, -5.5499e-02, -1.0812e-01,  7.4704e-02,  9.8012e-02],\n",
            "          [ 1.5725e-01, -8.0408e-02, -8.0926e-02,  1.0462e-01, -1.8994e-01],\n",
            "          [ 8.1602e-02,  6.6408e-02, -5.7377e-02, -1.8729e-01,  1.0850e-04]]],\n",
            "\n",
            "\n",
            "        [[[-1.6702e-01,  3.2249e-02, -1.4538e-01,  1.6212e-02, -1.7077e-01],\n",
            "          [ 1.5791e-01, -1.0005e-01,  6.0738e-02, -1.9526e-02, -2.1651e-02],\n",
            "          [ 1.1752e-01,  1.6462e-01, -1.6470e-01,  1.0012e-01,  1.8501e-01],\n",
            "          [ 1.2369e-01, -2.0926e-02, -1.8487e-01, -8.2118e-02, -1.5325e-01],\n",
            "          [ 5.4378e-02,  1.0574e-01,  9.4337e-02,  7.3320e-03,  1.2995e-01]]]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([ 0.1214, -0.0163, -0.0604,  0.1471, -0.1376,  0.1700],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[[[-0.0170,  0.0413,  0.0589,  0.0267,  0.0057],\n",
            "          [ 0.0016,  0.0400,  0.0776,  0.0310,  0.0777],\n",
            "          [ 0.0484, -0.0109, -0.0244,  0.0391, -0.0554],\n",
            "          [-0.0665, -0.0392,  0.0071, -0.0201, -0.0026],\n",
            "          [ 0.0427,  0.0470,  0.0198, -0.0650,  0.0760]],\n",
            "\n",
            "         [[ 0.0529,  0.0117,  0.0518, -0.0153, -0.0130],\n",
            "          [-0.0629, -0.0642, -0.0510, -0.0634, -0.0777],\n",
            "          [ 0.0616, -0.0768, -0.0097, -0.0248,  0.0108],\n",
            "          [-0.0231, -0.0065,  0.0762, -0.0277,  0.0586],\n",
            "          [ 0.0067,  0.0578,  0.0350, -0.0084, -0.0708]],\n",
            "\n",
            "         [[ 0.0431,  0.0114, -0.0571, -0.0774,  0.0085],\n",
            "          [-0.0211,  0.0073, -0.0029, -0.0736,  0.0126],\n",
            "          [-0.0192,  0.0287, -0.0247, -0.0769,  0.0400],\n",
            "          [-0.0708,  0.0564, -0.0664, -0.0163, -0.0111],\n",
            "          [-0.0308, -0.0642, -0.0653, -0.0065,  0.0165]],\n",
            "\n",
            "         [[-0.0342,  0.0266, -0.0073, -0.0332,  0.0147],\n",
            "          [ 0.0032,  0.0059,  0.0329,  0.0370,  0.0318],\n",
            "          [ 0.0428,  0.0365, -0.0581, -0.0635,  0.0578],\n",
            "          [-0.0369, -0.0613,  0.0237,  0.0699, -0.0732],\n",
            "          [-0.0604, -0.0320, -0.0286,  0.0778,  0.0566]],\n",
            "\n",
            "         [[ 0.0006,  0.0411,  0.0226, -0.0642,  0.0142],\n",
            "          [-0.0347,  0.0557, -0.0495, -0.0575, -0.0695],\n",
            "          [ 0.0372, -0.0391, -0.0592, -0.0650,  0.0207],\n",
            "          [-0.0667, -0.0513, -0.0569,  0.0312,  0.0083],\n",
            "          [-0.0416, -0.0245,  0.0333, -0.0245, -0.0486]],\n",
            "\n",
            "         [[ 0.0717, -0.0368, -0.0649,  0.0077,  0.0493],\n",
            "          [ 0.0665, -0.0483, -0.0765,  0.0750,  0.0244],\n",
            "          [-0.0580,  0.0302, -0.0750,  0.0748, -0.0575],\n",
            "          [-0.0126,  0.0212, -0.0228,  0.0122,  0.0658],\n",
            "          [ 0.0438,  0.0237,  0.0257,  0.0058,  0.0536]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0523,  0.0744, -0.0065, -0.0348,  0.0421],\n",
            "          [-0.0720,  0.0656, -0.0474, -0.0046,  0.0394],\n",
            "          [ 0.0632, -0.0764,  0.0525, -0.0077, -0.0677],\n",
            "          [ 0.0414,  0.0744, -0.0505,  0.0699,  0.0807],\n",
            "          [ 0.0668, -0.0403,  0.0251, -0.0507,  0.0434]],\n",
            "\n",
            "         [[ 0.0088, -0.0422,  0.0747,  0.0768,  0.0748],\n",
            "          [ 0.0555,  0.0316, -0.0460, -0.0642,  0.0035],\n",
            "          [-0.0017,  0.0477,  0.0052,  0.0807,  0.0379],\n",
            "          [ 0.0021, -0.0756,  0.0364,  0.0305, -0.0035],\n",
            "          [ 0.0085, -0.0300, -0.0078, -0.0214, -0.0741]],\n",
            "\n",
            "         [[ 0.0718, -0.0632,  0.0236, -0.0021, -0.0023],\n",
            "          [ 0.0251, -0.0662,  0.0390,  0.0784,  0.0366],\n",
            "          [-0.0687,  0.0746, -0.0507,  0.0644,  0.0729],\n",
            "          [ 0.0688, -0.0795,  0.0029, -0.0233, -0.0812],\n",
            "          [-0.0364, -0.0374,  0.0350,  0.0716,  0.0468]],\n",
            "\n",
            "         [[-0.0401, -0.0446,  0.0333, -0.0378, -0.0369],\n",
            "          [-0.0019,  0.0669,  0.0698, -0.0729,  0.0410],\n",
            "          [ 0.0794, -0.0189, -0.0289, -0.0740,  0.0321],\n",
            "          [ 0.0110, -0.0301, -0.0210,  0.0367, -0.0636],\n",
            "          [ 0.0677, -0.0064,  0.0108, -0.0127, -0.0264]],\n",
            "\n",
            "         [[-0.0070,  0.0328,  0.0445,  0.0194, -0.0140],\n",
            "          [-0.0629,  0.0476,  0.0049,  0.0430, -0.0327],\n",
            "          [ 0.0107,  0.0470,  0.0739,  0.0566, -0.0120],\n",
            "          [ 0.0204, -0.0515, -0.0623,  0.0041,  0.0376],\n",
            "          [-0.0365,  0.0778, -0.0593,  0.0606,  0.0026]],\n",
            "\n",
            "         [[-0.0121, -0.0193, -0.0806,  0.0020, -0.0118],\n",
            "          [-0.0776,  0.0436, -0.0716, -0.0337,  0.0148],\n",
            "          [ 0.0590, -0.0296, -0.0371,  0.0023, -0.0630],\n",
            "          [ 0.0071, -0.0487, -0.0668, -0.0541, -0.0569],\n",
            "          [ 0.0538,  0.0455,  0.0673,  0.0755, -0.0775]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0463,  0.0216, -0.0491,  0.0358,  0.0631],\n",
            "          [ 0.0662, -0.0359,  0.0052,  0.0756,  0.0772],\n",
            "          [ 0.0594,  0.0749,  0.0517, -0.0668,  0.0803],\n",
            "          [ 0.0602,  0.0557, -0.0445, -0.0325,  0.0695],\n",
            "          [-0.0145, -0.0625,  0.0635, -0.0661, -0.0238]],\n",
            "\n",
            "         [[ 0.0260, -0.0686, -0.0798,  0.0570, -0.0264],\n",
            "          [ 0.0021,  0.0241,  0.0614,  0.0073,  0.0048],\n",
            "          [ 0.0119, -0.0183,  0.0044,  0.0459, -0.0031],\n",
            "          [ 0.0209,  0.0303,  0.0460, -0.0234, -0.0643],\n",
            "          [ 0.0187,  0.0473, -0.0069,  0.0444, -0.0646]],\n",
            "\n",
            "         [[ 0.0075,  0.0680, -0.0193,  0.0146, -0.0301],\n",
            "          [-0.0720,  0.0408, -0.0151, -0.0475, -0.0234],\n",
            "          [-0.0688,  0.0628,  0.0810, -0.0423,  0.0190],\n",
            "          [-0.0625,  0.0575,  0.0404,  0.0614, -0.0675],\n",
            "          [-0.0482, -0.0776,  0.0479, -0.0031,  0.0276]],\n",
            "\n",
            "         [[ 0.0067, -0.0498, -0.0246,  0.0805, -0.0814],\n",
            "          [-0.0610, -0.0204,  0.0797, -0.0098, -0.0127],\n",
            "          [ 0.0791, -0.0740,  0.0482, -0.0141,  0.0256],\n",
            "          [ 0.0456, -0.0705,  0.0771,  0.0059, -0.0470],\n",
            "          [ 0.0562,  0.0457, -0.0762,  0.0072, -0.0270]],\n",
            "\n",
            "         [[ 0.0385,  0.0429,  0.0041,  0.0611, -0.0265],\n",
            "          [ 0.0392, -0.0385, -0.0491,  0.0291,  0.0360],\n",
            "          [-0.0366, -0.0405, -0.0670,  0.0417, -0.0652],\n",
            "          [-0.0555,  0.0513, -0.0577,  0.0726,  0.0619],\n",
            "          [-0.0319,  0.0535, -0.0720, -0.0366,  0.0750]],\n",
            "\n",
            "         [[ 0.0756, -0.0185,  0.0258, -0.0096, -0.0627],\n",
            "          [-0.0603, -0.0248,  0.0511, -0.0240,  0.0349],\n",
            "          [-0.0410, -0.0596,  0.0506, -0.0128,  0.0735],\n",
            "          [ 0.0134, -0.0618,  0.0020,  0.0417, -0.0489],\n",
            "          [-0.0492, -0.0701,  0.0067,  0.0165, -0.0032]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0811, -0.0361,  0.0741,  0.0037, -0.0060],\n",
            "          [ 0.0533,  0.0071,  0.0072, -0.0809, -0.0558],\n",
            "          [ 0.0555,  0.0475,  0.0749,  0.0636,  0.0543],\n",
            "          [ 0.0440,  0.0480, -0.0499,  0.0433,  0.0305],\n",
            "          [ 0.0346,  0.0650,  0.0041, -0.0172,  0.0518]],\n",
            "\n",
            "         [[ 0.0161,  0.0637, -0.0372,  0.0806,  0.0603],\n",
            "          [-0.0518,  0.0200,  0.0185, -0.0272,  0.0734],\n",
            "          [ 0.0274, -0.0584,  0.0259,  0.0704, -0.0652],\n",
            "          [-0.0522,  0.0148,  0.0273, -0.0609,  0.0388],\n",
            "          [ 0.0400, -0.0334,  0.0110,  0.0022, -0.0331]],\n",
            "\n",
            "         [[-0.0134,  0.0058, -0.0605, -0.0548,  0.0771],\n",
            "          [ 0.0687,  0.0721, -0.0455, -0.0405, -0.0752],\n",
            "          [ 0.0375,  0.0051,  0.0734,  0.0191,  0.0383],\n",
            "          [ 0.0075, -0.0350,  0.0766,  0.0567,  0.0682],\n",
            "          [-0.0661, -0.0500,  0.0248,  0.0227, -0.0401]],\n",
            "\n",
            "         [[ 0.0599, -0.0802,  0.0532, -0.0200, -0.0464],\n",
            "          [ 0.0452, -0.0424,  0.0143, -0.0716,  0.0676],\n",
            "          [ 0.0364, -0.0042, -0.0105,  0.0562, -0.0767],\n",
            "          [ 0.0793, -0.0649, -0.0700, -0.0393,  0.0347],\n",
            "          [-0.0183,  0.0246,  0.0003,  0.0291,  0.0361]],\n",
            "\n",
            "         [[-0.0472,  0.0343,  0.0715, -0.0272,  0.0600],\n",
            "          [-0.0097, -0.0196, -0.0798,  0.0527, -0.0039],\n",
            "          [-0.0046,  0.0016,  0.0699,  0.0130, -0.0523],\n",
            "          [-0.0544, -0.0412, -0.0580,  0.0477,  0.0050],\n",
            "          [ 0.0754,  0.0750,  0.0362, -0.0348,  0.0289]],\n",
            "\n",
            "         [[-0.0769, -0.0416, -0.0422, -0.0209,  0.0621],\n",
            "          [-0.0437, -0.0028, -0.0092,  0.0742, -0.0580],\n",
            "          [ 0.0692,  0.0775,  0.0043,  0.0686, -0.0428],\n",
            "          [ 0.0311,  0.0172, -0.0095, -0.0422,  0.0147],\n",
            "          [ 0.0472, -0.0487, -0.0357,  0.0086, -0.0250]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0687, -0.0711, -0.0228,  0.0517, -0.0368],\n",
            "          [-0.0805, -0.0409, -0.0151,  0.0005, -0.0459],\n",
            "          [-0.0002,  0.0121,  0.0620, -0.0808, -0.0513],\n",
            "          [-0.0688, -0.0082, -0.0426, -0.0778, -0.0522],\n",
            "          [-0.0293,  0.0510, -0.0802, -0.0020, -0.0535]],\n",
            "\n",
            "         [[ 0.0677,  0.0194, -0.0531, -0.0639, -0.0193],\n",
            "          [ 0.0560,  0.0351,  0.0188,  0.0789, -0.0380],\n",
            "          [ 0.0706,  0.0541,  0.0418,  0.0530,  0.0004],\n",
            "          [ 0.0482, -0.0461, -0.0165, -0.0127,  0.0154],\n",
            "          [ 0.0064,  0.0209, -0.0645,  0.0412,  0.0083]],\n",
            "\n",
            "         [[-0.0150, -0.0541, -0.0008, -0.0190,  0.0262],\n",
            "          [ 0.0777,  0.0741,  0.0432, -0.0625,  0.0128],\n",
            "          [-0.0179,  0.0719,  0.0114, -0.0250,  0.0219],\n",
            "          [ 0.0393, -0.0249, -0.0011,  0.0500,  0.0371],\n",
            "          [ 0.0082,  0.0239,  0.0414, -0.0145, -0.0172]],\n",
            "\n",
            "         [[ 0.0509, -0.0726,  0.0601, -0.0036, -0.0554],\n",
            "          [ 0.0204, -0.0279,  0.0511,  0.0236,  0.0171],\n",
            "          [ 0.0423, -0.0757,  0.0356, -0.0051, -0.0627],\n",
            "          [ 0.0426,  0.0306,  0.0169,  0.0365, -0.0668],\n",
            "          [ 0.0149, -0.0155, -0.0679, -0.0734, -0.0419]],\n",
            "\n",
            "         [[-0.0353, -0.0404, -0.0505, -0.0645,  0.0790],\n",
            "          [ 0.0179,  0.0176, -0.0563, -0.0757,  0.0612],\n",
            "          [ 0.0005, -0.0375,  0.0165, -0.0632, -0.0277],\n",
            "          [ 0.0442,  0.0531, -0.0304,  0.0191,  0.0661],\n",
            "          [ 0.0162, -0.0607, -0.0221,  0.0759, -0.0668]],\n",
            "\n",
            "         [[ 0.0338,  0.0747,  0.0136,  0.0737, -0.0537],\n",
            "          [-0.0757,  0.0659,  0.0609, -0.0488,  0.0040],\n",
            "          [ 0.0613,  0.0490,  0.0805, -0.0767,  0.0559],\n",
            "          [ 0.0545, -0.0386, -0.0134,  0.0598,  0.0145],\n",
            "          [-0.0655,  0.0629, -0.0669, -0.0091, -0.0790]]],\n",
            "\n",
            "\n",
            "        [[[-0.0165, -0.0806,  0.0732, -0.0147,  0.0339],\n",
            "          [-0.0269,  0.0621, -0.0214, -0.0100, -0.0002],\n",
            "          [-0.0286, -0.0532, -0.0575, -0.0631, -0.0504],\n",
            "          [-0.0746, -0.0503,  0.0517, -0.0630,  0.0777],\n",
            "          [-0.0811, -0.0446, -0.0122,  0.0241, -0.0148]],\n",
            "\n",
            "         [[-0.0666,  0.0091, -0.0287, -0.0081, -0.0046],\n",
            "          [-0.0644,  0.0500,  0.0112,  0.0389, -0.0110],\n",
            "          [ 0.0253,  0.0089,  0.0396, -0.0358, -0.0423],\n",
            "          [-0.0288, -0.0722,  0.0568,  0.0529,  0.0491],\n",
            "          [ 0.0040,  0.0288,  0.0635,  0.0128, -0.0484]],\n",
            "\n",
            "         [[ 0.0624,  0.0172, -0.0339, -0.0225, -0.0155],\n",
            "          [-0.0104, -0.0401, -0.0285, -0.0055, -0.0372],\n",
            "          [ 0.0649,  0.0648,  0.0152, -0.0405, -0.0481],\n",
            "          [-0.0512, -0.0412, -0.0237,  0.0257,  0.0421],\n",
            "          [-0.0248, -0.0792,  0.0505,  0.0440, -0.0329]],\n",
            "\n",
            "         [[ 0.0536,  0.0058, -0.0229, -0.0272,  0.0159],\n",
            "          [-0.0241, -0.0619,  0.0580,  0.0647,  0.0589],\n",
            "          [ 0.0638, -0.0336,  0.0775, -0.0326, -0.0053],\n",
            "          [ 0.0024,  0.0679, -0.0225, -0.0493,  0.0674],\n",
            "          [ 0.0676,  0.0726, -0.0355, -0.0492, -0.0634]],\n",
            "\n",
            "         [[-0.0185, -0.0496,  0.0340,  0.0272,  0.0192],\n",
            "          [ 0.0300,  0.0816, -0.0161, -0.0108,  0.0621],\n",
            "          [-0.0792,  0.0454, -0.0428, -0.0623,  0.0329],\n",
            "          [-0.0723, -0.0509,  0.0504,  0.0354,  0.0044],\n",
            "          [-0.0492,  0.0258,  0.0388, -0.0747, -0.0474]],\n",
            "\n",
            "         [[-0.0040, -0.0146, -0.0626, -0.0126, -0.0672],\n",
            "          [ 0.0465, -0.0167,  0.0759,  0.0330,  0.0632],\n",
            "          [-0.0691,  0.0667, -0.0382, -0.0146, -0.0171],\n",
            "          [-0.0022,  0.0031,  0.0744,  0.0794,  0.0095],\n",
            "          [-0.0553, -0.0093, -0.0359,  0.0697, -0.0246]]]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.0708,  0.0164,  0.0593, -0.0053,  0.0410,  0.0038, -0.0106,  0.0117,\n",
            "        -0.0760, -0.0186, -0.0607, -0.0599, -0.0323, -0.0150, -0.0035,  0.0596],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[-0.0121,  0.0401,  0.0205,  ..., -0.0499, -0.0065,  0.0185],\n",
            "        [ 0.0288, -0.0361,  0.0109,  ...,  0.0146, -0.0338,  0.0044],\n",
            "        [ 0.0265, -0.0136, -0.0103,  ...,  0.0229, -0.0460,  0.0211],\n",
            "        ...,\n",
            "        [ 0.0425,  0.0197, -0.0370,  ..., -0.0448,  0.0467,  0.0093],\n",
            "        [ 0.0381,  0.0310, -0.0491,  ...,  0.0027, -0.0020, -0.0405],\n",
            "        [ 0.0298, -0.0252,  0.0475,  ..., -0.0455, -0.0136,  0.0448]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0342,  0.0342,  0.0485,  0.0058,  0.0484,  0.0430, -0.0249,  0.0439,\n",
            "        -0.0289, -0.0163, -0.0165, -0.0239,  0.0195, -0.0008, -0.0143, -0.0145,\n",
            "        -0.0174,  0.0156,  0.0159,  0.0066,  0.0251,  0.0165, -0.0491,  0.0133,\n",
            "         0.0203, -0.0237, -0.0125,  0.0263, -0.0014, -0.0158,  0.0468,  0.0468,\n",
            "         0.0384,  0.0348,  0.0158, -0.0129, -0.0166, -0.0491, -0.0205,  0.0019,\n",
            "         0.0497, -0.0032, -0.0317, -0.0039,  0.0036, -0.0197,  0.0080, -0.0141,\n",
            "        -0.0191,  0.0138, -0.0021, -0.0067,  0.0047,  0.0132, -0.0137, -0.0053,\n",
            "         0.0337,  0.0481,  0.0312,  0.0319,  0.0440, -0.0046,  0.0401, -0.0364,\n",
            "        -0.0235, -0.0186, -0.0063, -0.0390,  0.0331, -0.0410,  0.0037,  0.0092,\n",
            "        -0.0069, -0.0177, -0.0079,  0.0139,  0.0425,  0.0323, -0.0361, -0.0460,\n",
            "         0.0264, -0.0218, -0.0053, -0.0065, -0.0025,  0.0187,  0.0097,  0.0357,\n",
            "         0.0351, -0.0100, -0.0077, -0.0207,  0.0448, -0.0021, -0.0018,  0.0343,\n",
            "        -0.0370, -0.0393,  0.0305, -0.0288,  0.0235,  0.0287,  0.0443,  0.0263,\n",
            "        -0.0178,  0.0064, -0.0235,  0.0188, -0.0058,  0.0282,  0.0475,  0.0187,\n",
            "        -0.0369,  0.0349,  0.0228,  0.0142, -0.0062, -0.0196, -0.0060,  0.0001],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([[ 0.0154, -0.0126, -0.0335,  ..., -0.0005, -0.0882, -0.0209],\n",
            "        [-0.0111, -0.0165, -0.0821,  ..., -0.0223,  0.0088,  0.0121],\n",
            "        [ 0.0393,  0.0650, -0.0801,  ..., -0.0826,  0.0261,  0.0191],\n",
            "        ...,\n",
            "        [-0.0131,  0.0537,  0.0489,  ..., -0.0217, -0.0844,  0.0643],\n",
            "        [-0.0018,  0.0363,  0.0287,  ..., -0.0240, -0.0127,  0.0680],\n",
            "        [-0.0502,  0.0882,  0.0634,  ..., -0.0653,  0.0177,  0.0796]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([-0.0687, -0.0312,  0.0811,  0.0240,  0.0415,  0.0749, -0.0873,  0.0902,\n",
            "         0.0539, -0.0908, -0.0284, -0.0868,  0.0584, -0.0846, -0.0660,  0.0260,\n",
            "         0.0497, -0.0340, -0.0617, -0.0365, -0.0397,  0.0578, -0.0091, -0.0707,\n",
            "        -0.0061,  0.0354,  0.0305, -0.0715, -0.0821, -0.0495,  0.0381, -0.0892,\n",
            "        -0.0171, -0.0557, -0.0143, -0.0656,  0.0621,  0.0675,  0.0775, -0.0276,\n",
            "         0.0375, -0.0476,  0.0772,  0.0453, -0.0262, -0.0784,  0.0286,  0.0217,\n",
            "        -0.0102,  0.0507, -0.0697, -0.0695,  0.0852,  0.0190,  0.0324,  0.0512,\n",
            "        -0.0095,  0.0586,  0.0364, -0.0280,  0.0471, -0.0346,  0.0839, -0.0124,\n",
            "         0.0008,  0.0265,  0.0759, -0.0856, -0.0807,  0.0578, -0.0623,  0.0526,\n",
            "         0.0788,  0.0726, -0.0504,  0.0498,  0.0859,  0.0101, -0.0273,  0.0042,\n",
            "         0.0341,  0.0673,  0.0424, -0.0602], requires_grad=True), Parameter containing:\n",
            "tensor([[ 3.9574e-02,  8.3248e-03, -4.1415e-02,  3.8845e-03, -2.6541e-02,\n",
            "         -5.1085e-02,  1.0273e-01, -4.9917e-02,  2.2179e-02,  1.0634e-01,\n",
            "         -5.6329e-02, -1.0861e-01, -8.2201e-02, -1.5205e-02,  3.4156e-02,\n",
            "         -5.0949e-02, -2.1331e-02,  6.5083e-02, -3.2879e-02,  6.6619e-02,\n",
            "          4.6755e-02,  5.5522e-03,  2.9094e-02, -1.5957e-02, -8.2763e-02,\n",
            "          9.6359e-02,  3.5515e-02,  1.0514e-01,  1.0474e-01, -5.5830e-03,\n",
            "          9.7461e-02,  1.0787e-01, -6.6810e-03, -3.9941e-02, -2.1644e-02,\n",
            "         -5.9872e-02,  9.7182e-02,  3.5089e-02,  9.9768e-02,  7.7016e-02,\n",
            "          8.4129e-02, -7.3039e-02, -8.2225e-04,  5.0407e-02,  5.4907e-02,\n",
            "         -4.6901e-02,  4.0562e-02, -3.3823e-02, -1.7677e-03,  5.7689e-02,\n",
            "         -7.7577e-02,  1.1196e-02,  4.2997e-02,  9.2125e-02, -1.9125e-02,\n",
            "          3.1296e-02, -6.5686e-02,  3.5567e-02,  2.0265e-02, -5.3759e-02,\n",
            "          4.4723e-02,  7.6333e-03,  5.5322e-02, -7.8799e-02, -1.1698e-02,\n",
            "          2.2440e-02, -2.3225e-02, -1.7883e-02, -3.1444e-02,  2.5900e-02,\n",
            "         -4.7520e-02, -3.5074e-02, -2.5716e-02,  3.0317e-02, -9.2628e-03,\n",
            "         -4.5413e-02,  3.0158e-02, -6.9944e-02, -4.4650e-02,  7.7119e-02,\n",
            "          1.7419e-02, -6.7808e-02,  5.9867e-02,  8.3791e-02],\n",
            "        [-3.4342e-02, -2.7723e-02, -1.9715e-02, -3.7155e-02, -3.9135e-02,\n",
            "         -7.3548e-02,  5.9739e-02, -1.4975e-02,  3.0340e-02, -4.5124e-02,\n",
            "          7.7678e-02, -9.1848e-02,  2.4099e-02,  3.5023e-02,  6.7073e-02,\n",
            "          4.3405e-02, -8.8916e-02,  2.6659e-02,  9.7774e-02,  9.4184e-02,\n",
            "          5.5644e-02, -8.6629e-02, -6.8170e-03,  9.7038e-02,  3.8905e-02,\n",
            "         -2.7819e-02, -5.2517e-02,  8.9525e-02, -8.0749e-02, -4.2287e-02,\n",
            "          9.1922e-02,  9.4394e-02,  6.5119e-02,  6.5886e-02,  3.4696e-02,\n",
            "         -8.4375e-02,  4.0380e-02,  6.8337e-02,  8.1636e-02, -8.3311e-02,\n",
            "          2.1461e-02, -3.9646e-02,  7.5051e-02, -3.7972e-02, -4.2921e-02,\n",
            "          2.0058e-02,  9.9445e-02, -4.3161e-02, -8.6484e-02,  7.0189e-02,\n",
            "          1.8560e-02, -6.2264e-02,  4.1007e-02, -4.0595e-02, -1.0637e-01,\n",
            "         -8.1626e-02,  6.4965e-03, -5.5860e-02, -7.8283e-02,  6.6735e-02,\n",
            "         -6.5236e-02,  6.3672e-02, -3.9598e-02,  1.0316e-01, -1.9086e-02,\n",
            "         -5.2004e-02,  1.9814e-02, -2.8224e-02, -5.6244e-02, -9.8277e-02,\n",
            "         -1.0720e-01,  2.5431e-02,  7.2814e-02, -3.5672e-02,  3.9496e-02,\n",
            "          9.6156e-02,  3.9953e-02, -4.3690e-02,  2.1332e-02, -8.0239e-02,\n",
            "         -5.3874e-02,  9.2594e-02, -1.2597e-03,  9.9211e-02],\n",
            "        [-9.0743e-02, -2.4062e-02,  2.4146e-02, -1.0597e-02,  5.7402e-02,\n",
            "          1.2341e-04,  5.5331e-02,  5.1285e-02, -7.6836e-03, -9.1554e-02,\n",
            "          6.4087e-02, -1.0787e-02, -3.1270e-02,  1.4124e-03, -2.8655e-02,\n",
            "         -4.5501e-02,  3.1865e-02,  7.6150e-02,  1.6244e-02,  5.0715e-02,\n",
            "         -7.9146e-02, -1.2516e-02, -6.5540e-02, -7.8285e-02,  6.9720e-02,\n",
            "         -1.4172e-03, -4.2210e-02, -5.0976e-02,  1.0252e-01, -6.3058e-02,\n",
            "         -3.3235e-04, -4.5351e-02,  2.0418e-02,  1.0070e-01,  5.4527e-02,\n",
            "          4.7231e-02, -1.0741e-01, -4.7689e-02,  9.4153e-02,  7.1096e-02,\n",
            "          9.4655e-02, -4.3929e-02,  5.3664e-02,  2.0311e-02, -1.1213e-02,\n",
            "         -9.5605e-02,  9.5388e-02, -7.3029e-02, -8.0508e-02, -7.3385e-02,\n",
            "         -6.0781e-02, -7.1021e-02, -1.0802e-04, -5.6837e-02, -9.2648e-02,\n",
            "          1.4663e-02,  5.8599e-03, -1.0775e-01, -2.6575e-03,  1.0267e-01,\n",
            "         -8.4692e-03,  5.4576e-02, -1.0366e-01, -4.2192e-02,  7.5207e-02,\n",
            "         -3.0059e-02, -9.5364e-02,  1.0102e-01, -7.2321e-03,  7.5263e-02,\n",
            "         -6.4914e-02,  3.2454e-02,  2.0471e-02, -9.2988e-03, -1.7362e-02,\n",
            "         -4.1153e-02,  5.9876e-02,  5.9447e-02, -1.0837e-02,  6.6009e-03,\n",
            "         -8.5491e-02,  5.2043e-02, -1.0592e-01,  9.1429e-02],\n",
            "        [ 5.5538e-02, -3.4605e-02, -4.7505e-02, -9.5785e-02,  9.8113e-02,\n",
            "         -6.8955e-02, -7.4803e-02,  8.5286e-02,  4.6641e-03, -3.2794e-02,\n",
            "          5.7218e-02,  2.9731e-02,  2.7706e-02,  8.6299e-02,  1.2925e-02,\n",
            "          9.1339e-02,  2.9851e-02, -8.8729e-02,  8.8944e-02, -3.9721e-02,\n",
            "          3.1040e-02, -5.6484e-02, -5.6757e-02,  6.9637e-02,  1.0535e-01,\n",
            "          1.0826e-01, -8.7980e-02, -5.3596e-02, -6.8245e-02,  1.0471e-01,\n",
            "          1.0093e-01, -5.0071e-02,  2.7810e-02, -1.4215e-02,  5.7788e-02,\n",
            "          8.0077e-03, -9.4245e-02,  5.9895e-03, -9.1803e-02, -4.6903e-02,\n",
            "          1.0814e-01,  7.6722e-02, -1.0362e-01,  5.8555e-02, -1.9101e-02,\n",
            "          2.8135e-03,  1.0935e-04,  7.3628e-02, -2.8112e-02,  8.2770e-02,\n",
            "         -1.0158e-01, -1.5206e-04,  5.5600e-02, -8.1467e-02, -1.6997e-02,\n",
            "         -1.9132e-02, -9.0796e-02, -7.6791e-02,  4.4554e-02,  3.2932e-02,\n",
            "          3.0566e-02, -6.5680e-02, -7.0745e-02,  6.7932e-02,  9.8214e-02,\n",
            "         -9.5610e-02,  7.9823e-02, -8.9958e-02,  3.1378e-02, -2.5416e-02,\n",
            "         -1.0253e-01, -8.2718e-02,  2.7013e-02,  8.8305e-02,  1.0397e-02,\n",
            "          3.9319e-02,  1.0031e-01,  9.5181e-02, -5.6594e-02,  4.1742e-03,\n",
            "         -3.8396e-02, -9.7960e-02, -3.4912e-03, -1.1228e-02],\n",
            "        [ 3.9057e-02, -9.8691e-02, -5.0124e-02, -1.0117e-01, -6.7931e-02,\n",
            "          6.3367e-02, -6.6762e-02,  5.7565e-02,  2.8509e-02,  5.1747e-02,\n",
            "          8.6835e-02, -2.6347e-02, -7.6828e-02, -1.1954e-02,  6.0935e-02,\n",
            "          9.4876e-02, -6.5336e-02,  4.3082e-02, -6.8055e-02,  2.8213e-02,\n",
            "         -1.0400e-01, -9.5942e-02,  6.5186e-02, -1.0567e-01, -5.4625e-02,\n",
            "         -5.0197e-02,  7.9601e-02,  6.6789e-02, -7.7366e-02,  5.3440e-03,\n",
            "         -4.9833e-02,  7.1573e-02, -1.9588e-02, -9.8833e-02,  1.0236e-01,\n",
            "          1.0185e-01, -1.0145e-01,  2.8190e-02, -4.0691e-02, -6.3888e-02,\n",
            "          1.5137e-02, -6.9772e-02,  8.7348e-02,  4.6611e-02, -2.4319e-02,\n",
            "         -3.2535e-02,  4.1914e-02, -7.7404e-02,  1.0872e-01,  1.2158e-02,\n",
            "          1.7966e-02, -6.7317e-02, -8.2627e-02,  9.9208e-02, -6.4399e-02,\n",
            "         -5.7060e-02, -8.3146e-02,  1.0778e-01,  7.1724e-03,  2.2718e-02,\n",
            "          3.8016e-02, -7.4008e-02, -7.7582e-02, -8.2260e-02,  5.6608e-02,\n",
            "          6.0743e-02, -2.4234e-02, -2.6679e-02,  2.4392e-02, -9.1008e-02,\n",
            "         -3.4696e-03,  9.6989e-03,  5.9685e-02, -6.4427e-02,  4.3923e-02,\n",
            "         -3.3203e-02, -4.9548e-02, -7.3579e-02, -7.9996e-02,  8.2631e-02,\n",
            "          5.6316e-03, -6.8477e-03, -1.7403e-02,  6.4923e-03],\n",
            "        [ 2.8059e-02,  4.4669e-02,  9.9989e-02,  6.6136e-02, -4.2379e-03,\n",
            "          2.5781e-02,  2.4165e-02,  5.9570e-02,  4.5087e-02,  2.4271e-02,\n",
            "          5.5954e-02,  1.0280e-01, -3.9483e-02, -1.0692e-01,  8.1508e-02,\n",
            "         -9.1779e-02, -1.0554e-01,  7.6423e-02, -5.7806e-02,  7.8261e-03,\n",
            "         -4.5281e-02, -7.6764e-02,  2.0185e-02, -7.7152e-02, -4.7251e-02,\n",
            "          3.9994e-02, -2.7805e-02, -9.2862e-02, -8.1666e-02, -1.0258e-01,\n",
            "          6.7258e-02,  9.3003e-02, -9.6820e-02, -1.0584e-01,  7.1088e-02,\n",
            "         -2.0936e-02, -9.2934e-02,  2.3120e-02,  1.0593e-01, -1.7076e-03,\n",
            "          7.7292e-02,  1.0683e-01, -9.6425e-02, -1.0423e-01,  1.0201e-01,\n",
            "          5.7234e-02,  7.1415e-02, -8.3132e-02,  5.0313e-02, -5.1117e-02,\n",
            "         -7.1773e-02,  3.2327e-02, -3.7652e-02, -3.7615e-02, -6.5337e-02,\n",
            "          9.6781e-02, -1.9031e-02, -3.5784e-02,  6.2645e-02, -5.5235e-02,\n",
            "         -2.2110e-02, -1.2002e-02,  1.1559e-02, -9.2362e-02, -9.5346e-02,\n",
            "          9.8551e-02,  3.5346e-02, -1.3324e-02,  1.7226e-02, -1.0156e-01,\n",
            "          8.0864e-02, -7.1698e-03,  1.0024e-01,  8.4798e-02, -9.2484e-02,\n",
            "          8.4645e-03,  1.9976e-02,  2.0888e-02, -8.0013e-02, -1.3314e-02,\n",
            "          5.2924e-03,  7.8318e-02,  6.5911e-02, -5.9105e-02],\n",
            "        [-2.5644e-02,  7.7595e-02,  5.5324e-02,  9.8636e-02, -8.4878e-02,\n",
            "         -6.7779e-02,  4.7740e-02, -2.5218e-03, -4.5890e-02, -6.3136e-03,\n",
            "          8.0752e-02, -1.4309e-02,  6.6259e-02,  7.2215e-02,  5.2846e-02,\n",
            "         -9.6877e-02, -5.2645e-02, -8.7058e-02, -3.2415e-02, -1.0164e-01,\n",
            "          1.3229e-02,  3.9473e-02,  4.4899e-02, -1.0070e-01,  2.3070e-03,\n",
            "         -9.5638e-02,  5.3649e-02, -7.1561e-02,  4.7550e-02, -3.7468e-03,\n",
            "          5.1544e-03, -3.5316e-02, -3.1364e-02, -5.2297e-02, -3.2269e-02,\n",
            "         -8.6705e-02, -9.0066e-03, -3.4727e-04, -7.8774e-02, -8.8758e-02,\n",
            "          9.2547e-02, -8.2587e-03,  9.7411e-02,  1.2018e-02,  6.0047e-02,\n",
            "         -4.2444e-02,  1.4222e-02, -8.4785e-02,  4.0675e-02,  9.1490e-02,\n",
            "          1.0713e-01,  4.3346e-02, -7.3903e-02,  1.0240e-01,  3.9995e-02,\n",
            "         -5.1907e-02, -7.6033e-02,  1.0753e-01,  6.7448e-02,  6.5898e-02,\n",
            "          4.7280e-02,  4.0031e-02,  6.1343e-02,  4.6512e-02, -5.2359e-02,\n",
            "          6.7145e-02,  3.8074e-02,  9.1002e-02, -9.2299e-02,  1.0555e-01,\n",
            "         -1.8093e-02, -7.5352e-02, -6.9122e-02,  6.5982e-02, -4.7888e-02,\n",
            "         -9.2527e-02,  3.8237e-02,  7.9571e-02,  5.8202e-02, -1.7004e-02,\n",
            "          9.8961e-02,  4.1426e-03, -4.0596e-03, -9.0514e-02],\n",
            "        [-1.0848e-01, -3.0771e-02, -3.4928e-02, -3.0939e-02,  2.1680e-02,\n",
            "         -1.0056e-01,  2.0508e-02,  6.0464e-02, -3.7478e-02,  7.5905e-02,\n",
            "         -9.1472e-03,  2.3095e-02,  6.6160e-02, -6.8632e-02,  6.7228e-02,\n",
            "          9.4811e-02,  6.5697e-02,  3.5115e-02,  4.0017e-02,  4.6657e-02,\n",
            "         -9.0319e-02,  8.2031e-02,  9.4777e-02, -1.0863e-01, -8.3087e-03,\n",
            "          3.0765e-02,  7.2877e-03, -6.5205e-02, -8.6818e-02, -3.7198e-02,\n",
            "         -2.8734e-02, -9.9873e-02,  3.2479e-02,  7.6226e-02,  4.7336e-02,\n",
            "         -7.7321e-02,  6.4035e-03,  1.7994e-02,  8.1464e-02,  5.0351e-02,\n",
            "          1.4055e-02, -7.2608e-02, -7.3240e-02,  5.3817e-02, -6.7947e-02,\n",
            "          3.8249e-02,  6.0710e-02,  7.6119e-02,  1.0380e-02,  9.7598e-02,\n",
            "          8.0614e-02, -9.6974e-02, -6.3398e-02,  3.8735e-02,  5.2792e-03,\n",
            "          8.5781e-02,  1.4921e-02,  2.4833e-02, -1.0278e-01,  6.4305e-02,\n",
            "         -5.9660e-02,  5.7188e-02, -5.5563e-02,  6.6388e-02,  1.5517e-02,\n",
            "         -9.1462e-02, -7.5960e-02,  5.2579e-02, -9.1768e-02,  7.4256e-02,\n",
            "         -3.1632e-02, -5.1449e-02,  4.5806e-03, -2.8299e-02, -7.0313e-02,\n",
            "          3.8756e-02,  6.5918e-03,  1.4015e-02, -8.1113e-04, -1.0819e-01,\n",
            "          2.3524e-02, -1.7128e-02, -2.7597e-03, -3.8242e-02],\n",
            "        [ 3.3987e-02, -9.1754e-02,  2.1136e-02,  2.2276e-02,  6.6692e-02,\n",
            "          7.8605e-02, -8.2973e-04, -1.4367e-02, -6.9269e-02,  5.3531e-02,\n",
            "          1.0251e-01,  4.9701e-02,  1.1596e-02, -9.8191e-02,  6.0486e-02,\n",
            "          1.0076e-02,  5.7310e-03,  1.0492e-01, -7.5001e-03,  3.4644e-02,\n",
            "          1.3926e-02,  9.1330e-02, -3.8606e-02,  3.3999e-02, -7.5348e-02,\n",
            "          1.0376e-01,  8.8651e-02, -9.3548e-02, -8.5535e-02,  6.2384e-02,\n",
            "          4.2325e-02,  9.3917e-02, -1.0885e-02, -1.0125e-01, -5.1829e-02,\n",
            "          4.0684e-02, -1.3884e-02, -5.0041e-02,  8.1982e-02,  8.2720e-02,\n",
            "          9.0662e-02,  6.8722e-02, -4.9307e-02,  7.7488e-02,  8.0579e-02,\n",
            "         -8.5097e-02, -4.4074e-02, -3.3363e-02,  9.1478e-02,  9.5861e-02,\n",
            "         -5.4605e-02,  1.0620e-01,  4.8899e-02,  2.1498e-02, -9.8132e-02,\n",
            "          9.8520e-03,  1.5284e-02, -4.1045e-02,  5.9680e-02,  1.4149e-02,\n",
            "         -2.0394e-02,  7.8589e-02, -1.0753e-02, -5.0983e-03,  1.0360e-01,\n",
            "          2.0372e-02, -7.5249e-02,  9.8392e-02, -5.5120e-02, -9.7745e-02,\n",
            "         -8.4167e-02, -3.9975e-02, -6.4969e-02, -5.6369e-02, -7.7000e-02,\n",
            "         -8.9247e-02, -2.5954e-02,  5.7265e-02,  2.5960e-02, -8.6373e-02,\n",
            "          4.3175e-02,  6.6438e-02, -9.9624e-02,  2.0965e-02],\n",
            "        [-3.5441e-02,  8.4217e-02, -4.9214e-02,  6.2368e-02,  3.2102e-02,\n",
            "          5.8994e-02,  3.7522e-02,  8.4152e-02, -5.0707e-02,  2.9686e-02,\n",
            "          7.5373e-02,  8.2093e-02, -1.0592e-01, -7.4943e-02,  9.7444e-02,\n",
            "         -4.7864e-03, -7.8812e-02,  9.4268e-02, -1.7610e-03,  9.0636e-02,\n",
            "         -3.9455e-02, -6.6873e-02, -4.9156e-02,  4.4761e-02,  4.5319e-02,\n",
            "          3.6138e-02,  4.8176e-02,  7.4936e-02, -4.7103e-02, -1.5086e-02,\n",
            "          8.8410e-02, -1.8067e-02, -9.4361e-02, -2.3844e-02,  5.5689e-02,\n",
            "         -1.5785e-02,  9.0854e-02, -5.2367e-02, -6.2207e-02, -9.1093e-02,\n",
            "          2.1872e-03, -1.0392e-01, -2.9887e-02, -1.0516e-01, -6.8470e-02,\n",
            "          4.5177e-03,  1.5404e-02, -1.0096e-01, -1.0760e-01,  1.0903e-01,\n",
            "         -2.2035e-02,  2.8839e-02,  6.4268e-03,  2.0870e-02, -1.2595e-02,\n",
            "         -8.6321e-02, -4.0625e-02, -8.9161e-02, -2.6043e-02, -1.4022e-02,\n",
            "         -5.9052e-02, -3.1031e-03, -8.9917e-02, -6.8293e-02, -7.3215e-02,\n",
            "          6.4194e-02,  4.0899e-02, -2.6212e-02,  2.4830e-03,  3.9606e-02,\n",
            "          6.0143e-02,  9.3373e-02, -3.3430e-02, -1.2182e-02,  3.7562e-02,\n",
            "          4.6389e-02, -5.7841e-02, -8.5762e-04, -2.3774e-02,  1.1755e-02,\n",
            "         -4.6104e-02, -8.0704e-02,  3.3340e-02, -4.7503e-02]],\n",
            "       requires_grad=True), Parameter containing:\n",
            "tensor([ 0.0958, -0.0751, -0.0221, -0.0579, -0.0378, -0.0726, -0.0855, -0.0113,\n",
            "        -0.0903, -0.0350], requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j_BJsYPeWKW"
      },
      "source": [
        "Let\\'s try a random 32x32 input. Note: expected input size of this net\n",
        "(LeNet) is 32x32. To use this net on the MNIST dataset, please resize\n",
        "the images from the dataset to 32x32.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QnVUVVPXeWKX",
        "outputId": "5239fa0e-3442-4b3e-a786-a2945934fb42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1554, -0.0728, -0.0223, -0.0758, -0.0987, -0.0030, -0.0877, -0.0198,\n",
            "         -0.0635, -0.1065]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "input = torch.randn(1, 1, 32, 32)\n",
        "# 随机生成一个 batch：N=1，1 通道，32x32\n",
        "# randn 是标准正态分布\n",
        "\n",
        "out = net(input)\n",
        "# 前向计算：得到 (1,10) 的输出：10 类得分\n",
        "\n",
        "print(out)\n",
        "# 打印输出张量；grad_fn=<AddmmBackward0> 说明它在计算图里，可用于反传：AddmmBackward0：最后的线性层本质是 add + matrix-multiply 的组合，其反传节点叫这个名字"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgzDZxcaeWKX"
      },
      "source": [
        "Zero the gradient buffers of all parameters and backprops with random\n",
        "gradients:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1mqbb24_eWKY"
      },
      "outputs": [],
      "source": [
        "net.zero_grad()\n",
        "# 把所有参数的 .grad 清空/置零\n",
        "# 重要：PyTorch 默认“梯度累积”，不清零会叠加\n",
        "out.backward(torch.randn(1, 10))\n",
        "# 从 out 这个非标量张量做 backward 时，必须提供同形状的“上游梯度”\n",
        "# 这里用随机 (1,10) 作为 dL/dout 来演示反传能跑通\n",
        "# 反传后：各参数的 .grad 会被填充"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v79Qd6bLeWKY"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p><code>torch.nn</code> only supports mini-batches. The entire <code>torch.nn</code>package only supports inputs that are a mini-batch of samples, and nota single sample.For example, <code>nn.Conv2d</code> will take in a 4D Tensor of<code>nSamples x nChannels x Height x Width</code>.If you have a single sample, just use <code>input.unsqueeze(0)</code> to adda fake batch dimension.</p>\n",
        "\n",
        "</div>\n",
        "\n",
        "Before proceeding further, let\\'s recap all the classes you've seen so\n",
        "far.\n",
        "\n",
        "**Recap:**\n",
        "\n",
        ":   -   `torch.Tensor` - A *multi-dimensional array* with support for\n",
        "        autograd operations like `backward()`. Also *holds the gradient*\n",
        "        w.r.t. the tensor.\n",
        "    -   `nn.Module` - Neural network module. *Convenient way of\n",
        "        encapsulating parameters*, with helpers for moving them to GPU,\n",
        "        exporting, loading, etc.\n",
        "    -   `nn.Parameter` - A kind of Tensor, that is *automatically\n",
        "        registered as a parameter when assigned as an attribute to a*\n",
        "        `Module`.\n",
        "    -   `autograd.Function` - Implements *forward and backward\n",
        "        definitions of an autograd operation*. Every `Tensor` operation\n",
        "        creates at least a single `Function` node that connects to\n",
        "        functions that created a `Tensor` and *encodes its history*.\n",
        "\n",
        "**At this point, we covered:**\n",
        "\n",
        ":   -   Defining a neural network\n",
        "    -   Processing inputs and calling backward\n",
        "\n",
        "**Still Left:**\n",
        "\n",
        ":   -   Computing the loss\n",
        "    -   Updating the weights of the network\n",
        "\n",
        "Loss Function\n",
        "=============\n",
        "\n",
        "A loss function takes the (output, target) pair of inputs, and computes\n",
        "a value that estimates how far away the output is from the target.\n",
        "\n",
        "There are several different [loss\n",
        "functions](https://pytorch.org/docs/nn.html#loss-functions) under the nn\n",
        "package . A simple loss is: `nn.MSELoss` which computes the mean-squared\n",
        "error between the output and the target.\n",
        "\n",
        "For example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0OEpwvgfeWKZ",
        "outputId": "ad9b4c6c-b98f-4b70-a452-3421ac838204",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.2424, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "output = net(input)\n",
        "# 再跑一次前向，得到 (1,10)\n",
        "target = torch.randn(10)  # a dummy target, for example\n",
        "# 随机生成一个“假目标”，形状 (10,)\n",
        "target = target.view(1, -1)  # make it the same shape as output\n",
        "# reshape 成 (1,10)，以便和 output 对齐\n",
        "# -1 表示自动推断该维度大小\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "# 创建均方误差损失：mean((output - target)^2)\n",
        "\n",
        "loss = criterion(output, target)\n",
        "# 计算损失：得到一个标量 Tensor\n",
        "\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svKsLRMheWKc"
      },
      "source": [
        "Now, if you follow `loss` in the backward direction, using its\n",
        "`.grad_fn` attribute, you will see a graph of computations that looks\n",
        "like this:\n",
        "\n",
        "``` {.sh}\n",
        "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
        "      -> flatten -> linear -> relu -> linear -> relu -> linear\n",
        "      -> MSELoss\n",
        "      -> loss\n",
        "```\n",
        "\n",
        "So, when we call `loss.backward()`, the whole graph is differentiated\n",
        "w.r.t. the neural net parameters, and all Tensors in the graph that have\n",
        "`requires_grad=True` will have their `.grad` Tensor accumulated with the\n",
        "gradient.\n",
        "\n",
        "For illustration, let us follow a few steps backward:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qmx0X-bBeWKd",
        "outputId": "c09af4b7-2c65-40e4-f10c-8a865360fada",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<MseLossBackward0 object at 0x79421cc155d0>\n",
            "<AddmmBackward0 object at 0x79421eb269e0>\n",
            "<AccumulateGrad object at 0x79421eb269e0>\n"
          ]
        }
      ],
      "source": [
        "print(loss.grad_fn)  # MSELoss loss 的反向函数节点：MseLossBackward0\n",
        "print(loss.grad_fn.next_functions[0][0])  # Linear  next_functions 指向它的输入来自哪些节点，会看到与最后的线性层（AddmmBackward0）相关的节点\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU 继续往回追，会看到更前面的节点 AccumulateGrad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2lutYIMeWKe"
      },
      "source": [
        "Backprop\n",
        "========\n",
        "\n",
        "To backpropagate the error all we have to do is to `loss.backward()`.\n",
        "You need to clear the existing gradients though, else gradients will be\n",
        "accumulated to existing gradients.\n",
        "\n",
        "Now we shall call `loss.backward()`, and have a look at conv1\\'s bias\n",
        "gradients before and after the backward.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NFJuBDykeWKf",
        "outputId": "047b98f9-8577-4d70-b711-2131354902b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.bias.grad before backward\n",
            "None\n",
            "conv1.bias.grad after backward\n",
            "tensor([-0.0015,  0.0056, -0.0485,  0.0238, -0.0206,  0.0230])\n"
          ]
        }
      ],
      "source": [
        "net.zero_grad()     # zeroes the gradient buffers of all parameters 再次清零梯度，避免和之前的累积混在一起\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad)#反传前 conv1.bias.grad 通常是 None（还没计算/填充）\n",
        "\n",
        "loss.backward()#对 loss 做反传：把 d(loss)/d(param) 计算出来并累积到各 param.grad\n",
        "\n",
        "print('conv1.bias.grad after backward')\n",
        "print(net.conv1.bias.grad)#现在 conv1.bias.grad 会变成一个长度为 6 的向量（对应 6 个输出通道的 bias 梯度）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vcfPcMkeWKf"
      },
      "source": [
        "Now, we have seen how to use loss functions.\n",
        "\n",
        "**Read Later:**\n",
        "\n",
        "> The neural network package contains various modules and loss functions\n",
        "> that form the building blocks of deep neural networks. A full list\n",
        "> with documentation is [here](https://pytorch.org/docs/nn).\n",
        "\n",
        "**The only thing left to learn is:**\n",
        "\n",
        "> -   Updating the weights of the network\n",
        "\n",
        "Update the weights\n",
        "==================\n",
        "\n",
        "The simplest update rule used in practice is the Stochastic Gradient\n",
        "Descent (SGD):\n",
        "\n",
        "``` {.python}\n",
        "weight = weight - learning_rate * gradient\n",
        "```\n",
        "\n",
        "We can implement this using simple Python code:\n",
        "\n",
        "``` {.python}\n",
        "learning_rate = 0.01\n",
        "for f in net.parameters():\n",
        "    f.data.sub_(f.grad.data * learning_rate)\n",
        "```\n",
        "\n",
        "However, as you use neural networks, you want to use various different\n",
        "update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc. To enable\n",
        "this, we built a small package: `torch.optim` that implements all these\n",
        "methods. Using it is very simple:\n",
        "\n",
        "``` {.python}\n",
        "import torch.optim as optim\n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # zero the gradient buffers\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()    # Does the update\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fry98V1BeWKf"
      },
      "source": [
        "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
        "\n",
        "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
        "\n",
        "<p>Observe how gradient buffers had to be manually set to zero using<code>optimizer.zero_grad()</code>. This is because gradients are accumulatedas explained in the <a href=\"\">Backprop</a> section.</p>\n",
        "\n",
        "</div>\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}